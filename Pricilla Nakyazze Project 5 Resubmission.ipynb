{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85371170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark>=3.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de6e79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fff72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"MyApp\").getOrCreate()\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "966b79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Does JAVA path exist\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\"\n",
    "print(\"JAVA_HOME exists:\", os.path.exists(os.environ[\"JAVA_HOME\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e5875",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d369bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, when, monotonically_increasing_id\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "from pyspark.sql.functions import avg, lit\n",
    "from pyspark.sql.functions import col, when, monotonically_increasing_id, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "264cf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Create SparkSession (only once per notebook)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JesterDataLoader\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4faad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_df = spark.read.format('csv').load(r\"C:\\Users\\pricc\\OneDrive\\Desktop\\jester-data-1.csv\",  header=False,  inferSchema=True,  sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da564dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: int, _c1: double, _c2: double, _c3: double, _c4: double, _c5: double, _c6: double, _c7: double, _c8: double, _c9: double, _c10: double, _c11: double, _c12: double, _c13: double, _c14: double, _c15: double, _c16: double, _c17: double, _c18: double, _c19: double, _c20: double, _c21: double, _c22: double, _c23: double, _c24: double, _c25: double, _c26: double, _c27: double, _c28: double, _c29: double, _c30: double, _c31: double, _c32: double, _c33: double, _c34: double, _c35: double, _c36: double, _c37: double, _c38: double, _c39: double, _c40: double, _c41: double, _c42: double, _c43: double, _c44: double, _c45: double, _c46: double, _c47: double, _c48: double, _c49: double, _c50: double, _c51: double, _c52: double, _c53: double, _c54: double, _c55: double, _c56: double, _c57: double, _c58: double, _c59: double, _c60: double, _c61: double, _c62: double, _c63: double, _c64: double, _c65: double, _c66: double, _c67: double, _c68: double, _c69: double, _c70: double, _c71: double, _c72: double, _c73: double, _c74: double, _c75: double, _c76: double, _c77: double, _c78: double, _c79: double, _c80: double, _c81: double, _c82: double, _c83: double, _c84: double, _c85: double, _c86: double, _c87: double, _c88: double, _c89: double, _c90: double, _c91: double, _c92: double, _c93: double, _c94: double, _c95: double, _c96: double, _c97: double, _c98: double, _c99: double, _c100: double]\n"
     ]
    }
   ],
   "source": [
    "print(joke_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da5b0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Rename columns\n",
    "joke_cols = ['ratingCount'] + [f'joke_{i}' for i in range(1, 101)]\n",
    "joke_df = joke_df.toDF(*joke_cols)\n",
    "\n",
    "# Step 2: Drop the 'ratingCount' column\n",
    "joke_df = joke_df.drop('ratingCount')\n",
    "\n",
    "# Step 3: Replace 99 with null (None)\n",
    "for c in joke_df.columns:\n",
    "    joke_df = joke_df.withColumn(c, when(col(c) == 99, None).otherwise(col(c)))\n",
    "\n",
    "# Step 4: Limit to first 5000 users\n",
    "joke_df = joke_df.limit(5000)\n",
    "\n",
    "# Optional: Add userId column (if needed for indexing)\n",
    "joke_df = joke_df.withColumn(\"userId\", monotonically_increasing_id())\n",
    "\n",
    "# --- If you want a NumPy or sparse matrix ---\n",
    "# WARNING: This part requires collecting to driver. Not scalable beyond small data.\n",
    "ratings_np = np.array(joke_df.drop(\"userId\").collect())\n",
    "ratings_matrix = np.array([[float(val) if val is not None else np.nan for val in row] for row in ratings_np])\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "ratings_sparse = csr_matrix(np.nan_to_num(ratings_matrix, nan=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ec86682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.82  8.79 -9.66 ...   nan   nan   nan]\n",
      " [ 4.08 -0.29  6.36 ...  0.34 -4.32  1.07]\n",
      " [  nan   nan   nan ...   nan   nan   nan]\n",
      " ...\n",
      " [-0.68 -2.48 -3.4  ...   nan   nan   nan]\n",
      " [ 1.02 -3.16  3.16 ... -0.68 -6.6  -1.75]\n",
      " [ 3.54  2.82 -2.14 ...  1.31  0.87  5.29]]\n"
     ]
    }
   ],
   "source": [
    "print(ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1e47289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userId|jokeId|rating|\n",
      "+------+------+------+\n",
      "|     1|     0|  4.08|\n",
      "|     1|    12| -0.29|\n",
      "|     1|    23|  6.36|\n",
      "|     1|    34|  4.37|\n",
      "|     1|    45| -2.38|\n",
      "+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Identify rating columns (jokes)\n",
    "rating_cols = [c for c in joke_df.columns if c.startswith(\"joke_\")]\n",
    "\n",
    "# 2) Replace 99 with 0 (or missing)\n",
    "df = joke_df.limit(5000)  # limit rows if needed\n",
    "\n",
    "for c in rating_cols:\n",
    "    df = df.withColumn(c, when(col(c) == 99, 0.0).otherwise(col(c).cast(\"double\")))\n",
    "\n",
    "# 3) Add unique user ID column (rawUser)\n",
    "df = df.withColumn(\"rawUser\", monotonically_increasing_id())\n",
    "\n",
    "# 4) Convert wide → long format: (rawUser, rawJoke, rating)\n",
    "n = len(rating_cols)\n",
    "stack_expr = \", \".join([f\"'{c}', {c}\" for c in rating_cols])\n",
    "long_df = df.selectExpr(\"rawUser\", f\"stack({n}, {stack_expr}) as (rawJoke, rating)\")\n",
    "\n",
    "# 5) Convert joke string 'joke_42' → integer 42\n",
    "long_df = long_df.withColumn(\"rawJoke\", expr(\"int(substring(rawJoke, 6))\"))\n",
    "\n",
    "# 6) Index users and jokes to consecutive integers for ALS\n",
    "user_indexer = StringIndexer(inputCol=\"rawUser\", outputCol=\"userId\").fit(long_df)\n",
    "joke_indexer = StringIndexer(inputCol=\"rawJoke\", outputCol=\"jokeId\").fit(long_df)\n",
    "\n",
    "indexed = user_indexer.transform(long_df)\n",
    "indexed = joke_indexer.transform(indexed)\n",
    "\n",
    "# 7) Select and cast final columns, drop nulls\n",
    "data = indexed.select(\n",
    "    col(\"userId\").cast(IntegerType()),\n",
    "    col(\"jokeId\").cast(IntegerType()),\n",
    "    col(\"rating\").cast(\"double\")\n",
    ").na.drop()\n",
    "\n",
    "# 8) Remove rows where userId == 0\n",
    "data = data.filter(col(\"userId\") != 0)\n",
    "\n",
    "# Show first 5 rows of filtered data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0fb67",
   "metadata": {},
   "source": [
    "RMSE ALS TEST ON 5000 ratings\n",
    "\n",
    "I am asking Spark to randomly split the rows of the data DataFrame into two parts, 80% of the rows go into train and 20% go into test.\n",
    "This is done row-wise, not by user or item, meaning each (userId, jokeId, rating) row is randomly assigned to one of the two datasets according to the 80/20 ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f88dcc",
   "metadata": {},
   "source": [
    "Build an ALS model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9512b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS Model Performance :\n",
      "  RMSE_ALS: 4.5465\n",
      "  MSE_ALS:  20.6707\n",
      "  MAE_ALS:  3.5603\n"
     ]
    }
   ],
   "source": [
    "#split data\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize ALS model\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"  # drop NaN predictions during evaluation\n",
    ")\n",
    "\n",
    "#  Train ALS model\n",
    "model = als.fit(train)\n",
    "\n",
    "# Generate predictions on test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "#  Evaluate predictions with RMSE, MSE, MAE\n",
    "evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mse = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "rmse_als = evaluator_rmse.evaluate(predictions)\n",
    "mse_als = evaluator_mse.evaluate(predictions)\n",
    "mae_als = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "print(f\"ALS Model Performance :\")\n",
    "print(f\"  RMSE_ALS: {rmse_als:.4f}\")\n",
    "print(f\"  MSE_ALS:  {mse_als:.4f}\")\n",
    "print(f\"  MAE_ALS:  {mae_als:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe05458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model performance:\n",
      "  RMSE: 4.5465\n",
      "  Model time:      4.498 sec\n",
      "  Prediction time: 0.102 sec\n",
      "ALS model time / prediction time:\n",
      " 1  [4.498 sec / 0.102 sec]\n"
     ]
    }
   ],
   "source": [
    "start_model = time.time()\n",
    "\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "end_model = time.time()\n",
    "\n",
    "# Measure prediction time\n",
    "start_pred = time.time()\n",
    "predictions = model.transform(test)\n",
    "end_pred = time.time()\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Output results\n",
    "print(f\"ALS model performance:\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Model time:      {end_model - start_model:.3f} sec\")\n",
    "print(f\"  Prediction time: {end_pred - start_pred:.3f} sec\")\n",
    "print(f\"ALS model time / prediction time:\\n 1  [{end_model - start_model:.3f} sec / {end_pred - start_pred:.3f} sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d5116",
   "metadata": {},
   "source": [
    "BASELINE GLOBAL MEAN USING PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef25cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE (global mean): 5.1901\n"
     ]
    }
   ],
   "source": [
    "# Compute global average rating from training data\n",
    "mean_rating = train.select(avg(\"rating\")).first()[0]\n",
    "\n",
    "# Add that as a constant \"prediction\" column to test set\n",
    "baseline_preds = test.withColumn(\"prediction\", lit(mean_rating))\n",
    "\n",
    "# Evaluate RMSE\n",
    "baseline_rmse = RegressionEvaluator(\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ").evaluate(baseline_preds)\n",
    "\n",
    "print(f\"Baseline RMSE (global mean): {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422785a",
   "metadata": {},
   "source": [
    " Mean-Centering Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7048bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute user mean\n",
    "user_avg = train.groupBy(\"userId\").agg(avg(\"rating\").alias(\"mean_rating\"))\n",
    "\n",
    "# Join and mean-center ratings\n",
    "train_centered = train.join(user_avg, on=\"userId\")\n",
    "train_centered = train_centered.withColumn(\"rating_centered\", col(\"rating\") - col(\"mean_rating\"))\n",
    "\n",
    "# Train ALS on mean-centered data\n",
    "als_centered = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating_centered\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ").fit(train_centered)\n",
    "\n",
    "# Predict\n",
    "test_centered = test.join(user_avg, on=\"userId\")\n",
    "preds_centered = als_centered.transform(test_centered)\n",
    "preds_centered = preds_centered.withColumn(\"prediction\", col(\"prediction\") + col(\"mean_rating\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd40a7",
   "metadata": {},
   "source": [
    " Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cb14288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute user mean and stddev\n",
    "user_stats = train.groupBy(\"userId\").agg(\n",
    "    avg(\"rating\").alias(\"mean_rating\"),\n",
    "    stddev(\"rating\").alias(\"std_rating\")\n",
    ")\n",
    "\n",
    "# Join and z-score normalize\n",
    "train_z = train.join(user_stats, on=\"userId\")\n",
    "train_z = train_z.withColumn(\"rating_z\", (col(\"rating\") - col(\"mean_rating\")) / col(\"std_rating\"))\n",
    "\n",
    "# Train ALS on z-score data\n",
    "als_z = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating_z\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ").fit(train_z)\n",
    "\n",
    "# Predict and unnormalize\n",
    "test_z = test.join(user_stats, on=\"userId\")\n",
    "preds_z = als_z.transform(test_z)\n",
    "preds_z = preds_z.withColumn(\"prediction\", (col(\"prediction\") * col(\"std_rating\")) + col(\"mean_rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "086e964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Normalization: 4.546503340691266\n",
      "Mean Centered: 4.436330404001126\n",
      "Z-Score: 4.435515030094131\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "print(\"No Normalization:\", evaluator.evaluate(predictions))\n",
    "print(\"Mean Centered:\", evaluator.evaluate(preds_centered))\n",
    "print(\"Z-Score:\", evaluator.evaluate(preds_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ae0744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ALS Model Performance (no normalization):\n",
      "  RMSE_ALS: 4.5465\n",
      "  MSE_ALS:  20.6707\n",
      "  MAE_ALS:  3.5603\n",
      "ALS model time / prediction time:\n",
      " 1  [4.717 sec / 0.070 sec]\n",
      "\n",
      "ALS Model Performance (center):\n",
      "  RMSE_ALS: 4.3924\n",
      "  MSE_ALS:  19.2935\n",
      "  MAE_ALS:  3.3894\n",
      "ALS model time / prediction time:\n",
      " 1  [6.122 sec / 0.142 sec]\n",
      "\n",
      "ALS Model Performance (zscore):\n",
      "  RMSE_ALS: 0.9572\n",
      "  MSE_ALS:  0.9162\n",
      "  MAE_ALS:  0.7768\n",
      "ALS model time / prediction time:\n",
      " 1  [6.472 sec / 0.159 sec]\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(data, method=None):\n",
    "    \"\"\"\n",
    "    Normalize ratings using mean-centering or Z-score normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Spark DataFrame with columns ['userId', 'jokeId', 'rating']\n",
    "    - method: None, 'center', or 'zscore'\n",
    "\n",
    "    Returns:\n",
    "    - normalized Spark DataFrame\n",
    "    \"\"\"\n",
    "    if method is None:\n",
    "        return data\n",
    "\n",
    "    stats = data.groupBy(\"userId\").agg(\n",
    "        avg(\"rating\").alias(\"user_mean\"),\n",
    "        stddev(\"rating\").alias(\"user_std\")\n",
    "    )\n",
    "\n",
    "    joined = data.join(stats, on=\"userId\")\n",
    "\n",
    "    if method == 'center':\n",
    "        normed = joined.withColumn(\"rating\", col(\"rating\") - col(\"user_mean\"))\n",
    "    elif method == 'zscore':\n",
    "        normed = joined.withColumn(\"rating\", (col(\"rating\") - col(\"user_mean\")) / col(\"user_std\"))\n",
    "    else:\n",
    "        raise ValueError(\"Normalization must be 'center', 'zscore', or None\")\n",
    "\n",
    "    return normed.select(\"userId\", \"jokeId\", \"rating\").na.drop()\n",
    "\n",
    "def run_als_with_normalization(data, norm_method=None, maxIter=10, regParam=0.1):\n",
    "    \"\"\"\n",
    "    Train ALS with optional normalization and evaluate performance.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Spark DataFrame with ['userId', 'jokeId', 'rating']\n",
    "    - norm_method: None, 'center', or 'zscore'\n",
    "    \"\"\"\n",
    "    normalized_data = normalize_data(data, method=norm_method)\n",
    "    train, test = normalized_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    start_model = time.time()\n",
    "\n",
    "    als = ALS(\n",
    "        maxIter=maxIter,\n",
    "        regParam=regParam,\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"jokeId\",\n",
    "        ratingCol=\"rating\",\n",
    "        nonnegative=True,\n",
    "        coldStartStrategy=\"drop\"\n",
    "    )\n",
    "    model = als.fit(train)\n",
    "    end_model = time.time()\n",
    "\n",
    "    start_pred = time.time()\n",
    "    predictions = model.transform(test)\n",
    "    end_pred = time.time()\n",
    "\n",
    "    evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    evaluator_mse  = RegressionEvaluator(metricName=\"mse\",  labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    evaluator_mae  = RegressionEvaluator(metricName=\"mae\",  labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mse  = evaluator_mse.evaluate(predictions)\n",
    "    mae  = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "    print(f\"\\nALS Model Performance ({norm_method or 'no normalization'}):\")\n",
    "    print(f\"  RMSE_ALS: {rmse:.4f}\")\n",
    "    print(f\"  MSE_ALS:  {mse:.4f}\")\n",
    "    print(f\"  MAE_ALS:  {mae:.4f}\")\n",
    "    print(f\"ALS model time / prediction time:\\n 1  [{end_model - start_model:.3f} sec / {end_pred - start_pred:.3f} sec]\")\n",
    "\n",
    "# Run all three variants\n",
    "run_als_with_normalization(data, norm_method=None)\n",
    "run_als_with_normalization(data, norm_method=\"center\")\n",
    "run_als_with_normalization(data, norm_method=\"zscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c351d1",
   "metadata": {},
   "source": [
    "Matrix factorization-based collaborative filtering model using ALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12679cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collaborative filtering model using ALS \n",
      "  RMSE: 4.5524\n",
      "  MSE:  20.7245\n",
      "  MAE:  3.5627\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare data (wide → long)\n",
    "rating_cols = [c for c in joke_df.columns if c.startswith(\"joke_\")]\n",
    "df = joke_df.limit(5000)  # limit for speed\n",
    "\n",
    "# Replace 99s with 0.0 (or drop if you'd rather exclude them)\n",
    "for c in rating_cols:\n",
    "    df = df.withColumn(c, when(col(c) == 99, 0.0).otherwise(col(c).cast(\"double\")))\n",
    "\n",
    "df = df.withColumn(\"rawUser\", monotonically_increasing_id())\n",
    "\n",
    "# Wide to long format\n",
    "n = len(rating_cols)\n",
    "stack_expr = \", \".join([f\"'{c}', {c}\" for c in rating_cols])\n",
    "long_df = df.selectExpr(\"rawUser\", f\"stack({n}, {stack_expr}) as (rawJoke, rating)\")\n",
    "\n",
    "# Extract numeric joke ID\n",
    "long_df = long_df.withColumn(\"rawJoke\", expr(\"int(substring(rawJoke, 6))\"))\n",
    "\n",
    "# Index users and jokes\n",
    "user_indexer = StringIndexer(inputCol=\"rawUser\", outputCol=\"userId\").fit(long_df)\n",
    "joke_indexer = StringIndexer(inputCol=\"rawJoke\", outputCol=\"jokeId\").fit(long_df)\n",
    "\n",
    "indexed = user_indexer.transform(long_df)\n",
    "indexed = joke_indexer.transform(indexed)\n",
    "\n",
    "# Cast to int/double\n",
    "data = indexed.select(\n",
    "    col(\"userId\").cast(IntegerType()),\n",
    "    col(\"jokeId\").cast(IntegerType()),\n",
    "    col(\"rating\").cast(\"double\")\n",
    ").na.drop()\n",
    "\n",
    "# 3. Split into train/test\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 4. Train ALS model (collaborative filtering)\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating\",\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "# 5. Predict and evaluate\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse_cbf = evaluator.evaluate(predictions)\n",
    "evaluator_mse = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mse_cbf = evaluator_mse.evaluate(predictions)\n",
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae_cbf = evaluator_mae.evaluate(predictions)\n",
    "print(f\" Collaborative filtering model using ALS \")\n",
    "print(f\"  RMSE: {rmse_cbf:.4f}\")\n",
    "print(f\"  MSE:  {mse_cbf:.4f}\")\n",
    "print(f\"  MAE:  {mae_cbf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3946ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9d9163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(joke_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216d4e0",
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS Model Performance :\n",
      "  RMSE_ALS: 4.5465\n",
      "  MSE_ALS:  20.6707\n",
      "  MAE_ALS:  3.5603\n",
      "ALS model performance:\n",
      "  RMSE: 4.5465\n",
      "  Model time:      4.121 sec\n",
      "  Prediction time: 0.080 sec\n",
      "ALS model time / prediction time:\n",
      " 1  [4.121 sec / 0.080 sec]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ALS Model Performance :\")\n",
    "print(f\"  RMSE_ALS: {rmse_als:.4f}\")\n",
    "print(f\"  MSE_ALS:  {mse_als:.4f}\")\n",
    "print(f\"  MAE_ALS:  {mae_als:.4f}\")\n",
    "print(f\"ALS model performance:\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Model time:      {end_model - start_model:.3f} sec\")\n",
    "print(f\"  Prediction time: {end_pred - start_pred:.3f} sec\")\n",
    "print(f\"ALS model time / prediction time:\\n 1  [{end_model - start_model:.3f} sec / {end_pred - start_pred:.3f} sec]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd21072",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "The UBCF and SVD model in my project 4 performed better than the ALS as seen below.\n",
    "\n",
    "The ALS model metrics below are higher than the prior metrics from project 4. The ALS model time and prediction time is also higher.\n",
    "These metrics quantify how well a recommender system predicts user ratings, compared to actual ratings.\n",
    "\n",
    "ALS RMSE emphasizes large errors, so the Lower it is the better. ALS with no normalization uses raw ratings. Errors are relatively high at 4.565. Compared to the UBCF and SVD model in my project 4 with RMSE of 3.840 for UBCF and 3.2 for SVD. The Prediction time for ubcf was 0.090sec/0.128sec.  ALS assumes that all users rate on the same scale, which is rarely true, some users might always rate jokes lower/higher. The prediction time of the ALS models is very high ranging from 4.717 sec to 6.472 sec. See below other ALS models metrics and time.\n",
    "\n",
    "\n",
    "\n",
    "ALS Model Performance (no normalization):\n",
    "  RMSE_ALS: 4.5465,\n",
    "  MSE_ALS:  20.6707,\n",
    "  MAE_ALS:  3.5603,\n",
    "ALS model time / prediction time:\n",
    " 1  [4.717 sec / 0.070 sec],\n",
    "\n",
    "With Normalizing which is done by subtracting each user’s average rating.  We see a Slight improvement over raw data, The model now understands user bias better (e.g., User A may always rate jokes 2 points higher than User B). But the prediction time increases sue to preprocessing\n",
    "ALS Model Performance (center):\n",
    "  RMSE_ALS: 4.3924,\n",
    "  MSE_ALS:  19.2935,\n",
    "  MAE_ALS:  3.3894,\n",
    "ALS model time / prediction time:\n",
    " 1  [6.122 sec / 0.142 sec]\n",
    "\n",
    "Z-score captures both bias and variability of each user’s rating scale. This puts all users on a more comparable footing for ALS training. This model is offers best accuracy by far with slightly higher compute cost.\n",
    "\n",
    "ALS Model Performance (zscore):\n",
    "  RMSE_ALS: 0.9572,\n",
    "  MSE_ALS:  0.9162,\n",
    "  MAE_ALS:  0.7768,\n",
    "ALS model time / prediction time:\n",
    " 1  [6.472 sec / 0.159 sec]\n",
    "--------------------------------Project 4 metrics on Jester data --------------------\n",
    "Model   RMSE     MSE    MAE\n",
    "0  UBCF2  3.880  15.058  3.061,\n",
    "1   SVD2  3.206  10.279  2.418,\n",
    "2  IBCF2  4.251  18.069  3.395,\n",
    "\n",
    "MODEL TIME AND PREDICTION TIME COMPARISON and \n",
    "SVD run fold/sample [model time/prediction time] 1 [0.020sec/0.001sec],\n",
    "UBCF run fold/sample [model time/prediction time] 1 [0.090sec/0.128sec],\n",
    "IBCF run fold/sample [model time/prediction time] 1 [0.003sec/0.005sec],\n",
    "\n",
    "REGULARIZATION\n",
    "RegParam was applied automatically to both user and item latent factors.\n",
    "Spark’s ALS uses a conjugate gradient solver and optimizes in a distributed fashion.\n",
    "\n",
    "Spark does not expose fine-grained control like separate regularization for users/items or custom loss functions.  This leads to Optimization for large-scale distributed training, built-in regularization is efficient and easy to tune via regParam.\n",
    "Non-Spark Regularization often gives more control with reg_all, reg_bu, reg_bi for bias terms and separate regularization for user and item embeddings. This comes in handy for Easier experimentation, customization and is better suited for smaller datasets.\n",
    "\n",
    "It does not offer distributed scalability like Spark.\n",
    "\n",
    "With a PySpark/MLlib workflow your model lives in the JVM’s distributed memory (across executors), \n",
    "so after that one expensive fit() call you can call .transform() on new data very cheaply without retraining or pulling \n",
    "everything back to the driver. In contrast, a pure pandas-based pipeline typically retrains or recomputes locally for each batch\n",
    "(or requires you to keep everything in a global in-memory object),\n",
    "Pandas also don't automatically distribute work across a cluster.\n",
    "\n",
    "For a trained ALS (or any Spark ML) model, it stays “warm” in memory and can serve many prediction\n",
    "requests at scale, making pySpark a better choice for production deployment on large datasets or high-throughput scenarios.\n",
    "In conclusion, moving to a distributed architecture would seem advisable when data sets are large,\n",
    "processing is computationally demanding and / or minimizing processing time is critical.\n",
    "\n",
    "The process of setting up the enviroment and the PY, java and Pyspark versions that work together was such a headache. That put me against this model too.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
