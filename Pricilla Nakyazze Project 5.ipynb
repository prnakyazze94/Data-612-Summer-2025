{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85371170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark>=3.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "de6e79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3fff72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"MyApp\").getOrCreate()\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "966b79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Does JAVA path exist\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\"\n",
    "print(\"JAVA_HOME exists:\", os.path.exists(os.environ[\"JAVA_HOME\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e5875",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d369bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, when, monotonically_increasing_id\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "from pyspark.sql.functions import avg, lit\n",
    "from pyspark.sql.functions import col, when, monotonically_increasing_id, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "264cf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Create SparkSession (only once per notebook)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JesterDataLoader\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4faad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_df = spark.read.format('csv').load(r\"C:\\Users\\pricc\\OneDrive\\Desktop\\jester-data-1.csv\",  header=False,  inferSchema=True,  sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da564dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: int, _c1: double, _c2: double, _c3: double, _c4: double, _c5: double, _c6: double, _c7: double, _c8: double, _c9: double, _c10: double, _c11: double, _c12: double, _c13: double, _c14: double, _c15: double, _c16: double, _c17: double, _c18: double, _c19: double, _c20: double, _c21: double, _c22: double, _c23: double, _c24: double, _c25: double, _c26: double, _c27: double, _c28: double, _c29: double, _c30: double, _c31: double, _c32: double, _c33: double, _c34: double, _c35: double, _c36: double, _c37: double, _c38: double, _c39: double, _c40: double, _c41: double, _c42: double, _c43: double, _c44: double, _c45: double, _c46: double, _c47: double, _c48: double, _c49: double, _c50: double, _c51: double, _c52: double, _c53: double, _c54: double, _c55: double, _c56: double, _c57: double, _c58: double, _c59: double, _c60: double, _c61: double, _c62: double, _c63: double, _c64: double, _c65: double, _c66: double, _c67: double, _c68: double, _c69: double, _c70: double, _c71: double, _c72: double, _c73: double, _c74: double, _c75: double, _c76: double, _c77: double, _c78: double, _c79: double, _c80: double, _c81: double, _c82: double, _c83: double, _c84: double, _c85: double, _c86: double, _c87: double, _c88: double, _c89: double, _c90: double, _c91: double, _c92: double, _c93: double, _c94: double, _c95: double, _c96: double, _c97: double, _c98: double, _c99: double, _c100: double]\n"
     ]
    }
   ],
   "source": [
    "print(joke_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da5b0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Rename columns\n",
    "joke_cols = ['ratingCount'] + [f'joke_{i}' for i in range(1, 101)]\n",
    "joke_df = joke_df.toDF(*joke_cols)\n",
    "\n",
    "# Step 2: Drop the 'ratingCount' column\n",
    "joke_df = joke_df.drop('ratingCount')\n",
    "\n",
    "# Step 3: Replace 99 with null (None)\n",
    "for c in joke_df.columns:\n",
    "    joke_df = joke_df.withColumn(c, when(col(c) == 99, None).otherwise(col(c)))\n",
    "\n",
    "# Step 4: Limit to first 5000 users\n",
    "joke_df = joke_df.limit(5000)\n",
    "\n",
    "# Optional: Add userId column (if needed for indexing)\n",
    "joke_df = joke_df.withColumn(\"userId\", monotonically_increasing_id())\n",
    "\n",
    "# --- If you want a NumPy or sparse matrix ---\n",
    "# WARNING: This part requires collecting to driver. Not scalable beyond small data.\n",
    "ratings_np = np.array(joke_df.drop(\"userId\").collect())\n",
    "ratings_matrix = np.array([[float(val) if val is not None else np.nan for val in row] for row in ratings_np])\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "ratings_sparse = csr_matrix(np.nan_to_num(ratings_matrix, nan=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ec86682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.82  8.79 -9.66 ...   nan   nan   nan]\n",
      " [ 4.08 -0.29  6.36 ...  0.34 -4.32  1.07]\n",
      " [  nan   nan   nan ...   nan   nan   nan]\n",
      " ...\n",
      " [-0.68 -2.48 -3.4  ...   nan   nan   nan]\n",
      " [ 1.02 -3.16  3.16 ... -0.68 -6.6  -1.75]\n",
      " [ 3.54  2.82 -2.14 ...  1.31  0.87  5.29]]\n"
     ]
    }
   ],
   "source": [
    "print(ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1e47289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userId|jokeId|rating|\n",
      "+------+------+------+\n",
      "|     1|     0|  4.08|\n",
      "|     1|    12| -0.29|\n",
      "|     1|    23|  6.36|\n",
      "|     1|    34|  4.37|\n",
      "|     1|    45| -2.38|\n",
      "+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Identify rating columns (jokes)\n",
    "rating_cols = [c for c in joke_df.columns if c.startswith(\"joke_\")]\n",
    "\n",
    "# 2) Replace 99 with 0 (or missing)\n",
    "df = joke_df.limit(5000)  # limit rows if needed\n",
    "\n",
    "for c in rating_cols:\n",
    "    df = df.withColumn(c, when(col(c) == 99, 0.0).otherwise(col(c).cast(\"double\")))\n",
    "\n",
    "# 3) Add unique user ID column (rawUser)\n",
    "df = df.withColumn(\"rawUser\", monotonically_increasing_id())\n",
    "\n",
    "# 4) Convert wide → long format: (rawUser, rawJoke, rating)\n",
    "n = len(rating_cols)\n",
    "stack_expr = \", \".join([f\"'{c}', {c}\" for c in rating_cols])\n",
    "long_df = df.selectExpr(\"rawUser\", f\"stack({n}, {stack_expr}) as (rawJoke, rating)\")\n",
    "\n",
    "# 5) Convert joke string 'joke_42' → integer 42\n",
    "long_df = long_df.withColumn(\"rawJoke\", expr(\"int(substring(rawJoke, 6))\"))\n",
    "\n",
    "# 6) Index users and jokes to consecutive integers for ALS\n",
    "user_indexer = StringIndexer(inputCol=\"rawUser\", outputCol=\"userId\").fit(long_df)\n",
    "joke_indexer = StringIndexer(inputCol=\"rawJoke\", outputCol=\"jokeId\").fit(long_df)\n",
    "\n",
    "indexed = user_indexer.transform(long_df)\n",
    "indexed = joke_indexer.transform(indexed)\n",
    "\n",
    "# 7) Select and cast final columns, drop nulls\n",
    "data = indexed.select(\n",
    "    col(\"userId\").cast(IntegerType()),\n",
    "    col(\"jokeId\").cast(IntegerType()),\n",
    "    col(\"rating\").cast(\"double\")\n",
    ").na.drop()\n",
    "\n",
    "# 8) Remove rows where userId == 0\n",
    "data = data.filter(col(\"userId\") != 0)\n",
    "\n",
    "# Show first 5 rows of filtered data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0fb67",
   "metadata": {},
   "source": [
    "RMSE ALS TEST ON 5000 ratings\n",
    "\n",
    "I am asking Spark to randomly split the rows of the data DataFrame into two parts, 80% of the rows go into train and 20% go into test.\n",
    "This is done row-wise, not by user or item, meaning each (userId, jokeId, rating) row is randomly assigned to one of the two datasets according to the 80/20 ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f88dcc",
   "metadata": {},
   "source": [
    "Build an ALS model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9512b081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS Model Performance :\n",
      "  RMSE_ALS: 4.5555\n",
      "  MSE_ALS:  20.7528\n",
      "  MAE_ALS:  3.5661\n"
     ]
    }
   ],
   "source": [
    "#split data\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize ALS model\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"  # drop NaN predictions during evaluation\n",
    ")\n",
    "\n",
    "#  Train ALS model\n",
    "model = als.fit(train)\n",
    "\n",
    "# Generate predictions on test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "#  Evaluate predictions with RMSE, MSE, MAE\n",
    "evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mse = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "rmse_als = evaluator_rmse.evaluate(predictions)\n",
    "mse_als = evaluator_mse.evaluate(predictions)\n",
    "mae_als = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "print(f\"ALS Model Performance :\")\n",
    "print(f\"  RMSE_ALS: {rmse_als:.4f}\")\n",
    "print(f\"  MSE_ALS:  {mse_als:.4f}\")\n",
    "print(f\"  MAE_ALS:  {mae_als:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fe05458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model time / prediction time:\n",
      " 1  [2.722 sec / 0.048 sec]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ALS model time / prediction time:\\n 1  [{end_model - start_model:.3f} sec / {end_pred - start_pred:.3f} sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d5116",
   "metadata": {},
   "source": [
    "BASELINE GLOBAL MEAN USING PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ef25cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE (global mean): 5.1808\n"
     ]
    }
   ],
   "source": [
    "# Compute global average rating from training data\n",
    "mean_rating = train.select(avg(\"rating\")).first()[0]\n",
    "\n",
    "# Add that as a constant \"prediction\" column to test set\n",
    "baseline_preds = test.withColumn(\"prediction\", lit(mean_rating))\n",
    "\n",
    "# Evaluate RMSE\n",
    "baseline_rmse = RegressionEvaluator(\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ").evaluate(baseline_preds)\n",
    "\n",
    "print(f\"Baseline RMSE (global mean): {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c351d1",
   "metadata": {},
   "source": [
    "Matrix factorization-based collaborative filtering model using ALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "12679cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collaborative filtering model using ALS \n",
      "  RMSE: 4.5555\n",
      "  MSE:  20.7528\n",
      "  MAE:  3.5661\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare data (wide → long)\n",
    "rating_cols = [c for c in joke_df.columns if c.startswith(\"joke_\")]\n",
    "df = joke_df.limit(5000)  # limit for speed\n",
    "\n",
    "# Replace 99s with 0.0 (or drop if you'd rather exclude them)\n",
    "for c in rating_cols:\n",
    "    df = df.withColumn(c, when(col(c) == 99, 0.0).otherwise(col(c).cast(\"double\")))\n",
    "\n",
    "df = df.withColumn(\"rawUser\", monotonically_increasing_id())\n",
    "\n",
    "# Wide to long format\n",
    "n = len(rating_cols)\n",
    "stack_expr = \", \".join([f\"'{c}', {c}\" for c in rating_cols])\n",
    "long_df = df.selectExpr(\"rawUser\", f\"stack({n}, {stack_expr}) as (rawJoke, rating)\")\n",
    "\n",
    "# Extract numeric joke ID\n",
    "long_df = long_df.withColumn(\"rawJoke\", expr(\"int(substring(rawJoke, 6))\"))\n",
    "\n",
    "# Index users and jokes\n",
    "user_indexer = StringIndexer(inputCol=\"rawUser\", outputCol=\"userId\").fit(long_df)\n",
    "joke_indexer = StringIndexer(inputCol=\"rawJoke\", outputCol=\"jokeId\").fit(long_df)\n",
    "\n",
    "indexed = user_indexer.transform(long_df)\n",
    "indexed = joke_indexer.transform(indexed)\n",
    "\n",
    "# Cast to int/double\n",
    "data = indexed.select(\n",
    "    col(\"userId\").cast(IntegerType()),\n",
    "    col(\"jokeId\").cast(IntegerType()),\n",
    "    col(\"rating\").cast(\"double\")\n",
    ").na.drop()\n",
    "\n",
    "# 3. Split into train/test\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 4. Train ALS model (collaborative filtering)\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"jokeId\",\n",
    "    ratingCol=\"rating\",\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "# 5. Predict and evaluate\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse_cbf = evaluator.evaluate(predictions)\n",
    "evaluator_mse = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mse_cbf = evaluator_mse.evaluate(predictions)\n",
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae_cbf = evaluator_mae.evaluate(predictions)\n",
    "print(f\" Collaborative filtering model using ALS \")\n",
    "print(f\"  RMSE: {rmse_cbf:.4f}\")\n",
    "print(f\"  MSE:  {mse_cbf:.4f}\")\n",
    "print(f\"  MAE:  {mae_cbf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9d9163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(joke_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd21072",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "The UBCF and SVD model in my project 4 performed better than the ALS as seen below.\n",
    "\n",
    "The ALS model metrics below are higher than the prior metrics from project 4. The ALS model time and prediction time is also higher.\n",
    "\n",
    "\n",
    "ALS Model Performance :\n",
    "  RMSE_ALS: 4.5555\n",
    "  MSE_ALS:  20.7528\n",
    "  MAE_ALS:  3.5661\n",
    "\n",
    "ALS model time / prediction time:\n",
    " 1  [2.722 sec / 0.048 sec]\n",
    "--------------------------------Project 4 metrics on Jester data --------------------\n",
    "Model   RMSE     MSE    MAE\n",
    "0  UBCF2  3.880  15.058  3.061\n",
    "1   SVD2  3.206  10.279  2.418\n",
    "2  IBCF2  4.251  18.069  3.395\n",
    "\n",
    "MODEL TIME AND PREDICTION TIME COMPARISON\n",
    "\n",
    "SVD run fold/sample [model time/prediction time] 1 [0.020sec/0.001sec]\n",
    "UBCF run fold/sample [model time/prediction time] 1 [0.090sec/0.128sec]\n",
    "IBCF run fold/sample [model time/prediction time] 1 [0.003sec/0.005sec]\n",
    "\n",
    "\n",
    "ALS did not build / calculate the model any, however, ALS predictions were significantly faster \n",
    "than UBCF which scores high in Seredipity.\n",
    "\n",
    "With a PySpark/MLlib workflow your model lives in the JVM’s distributed memory (across executors), \n",
    "so after that one expensive fit() call you can call .transform() on new data very cheaply without retraining or pulling \n",
    "everything back to the driver. In contrast, a pure pandas-based pipeline typically retrains or recomputes locally for each batch\n",
    "(or requires you to keep everything in a global in-memory object),\n",
    "Pandas also don't automatically distribute work across a cluster.\n",
    "\n",
    "For a trained ALS (or any Spark ML) model, it stays “warm” in memory and can serve many prediction\n",
    "requests at scale, making pySpark a better choice for production deployment on large datasets or high-throughput scenarios.\n",
    "In conclusion, moving to a distributed architecture would seem advisable when data sets are large,\n",
    "processing is computationally demanding and / or minimizing processing time is critical.\n",
    "\n",
    "The process of setting up the enviroment and the PY, java and Pyspark versions that work together was such a headache. That put me against this model too.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
