{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e131bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "!pip install pyspark>=3.5.1\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "935f3c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME exists: True\n"
     ]
    }
   ],
   "source": [
    "#Does JAVA path exist\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\"\n",
    "print(\"JAVA_HOME exists:\", os.path.exists(os.environ[\"JAVA_HOME\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9478b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"MyApp\").getOrCreate()\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bec5892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME exists: True\n"
     ]
    }
   ],
   "source": [
    "#Does JAVA path exist\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\app.py\\Java\\jdk-11\"\n",
    "print(\"JAVA_HOME exists:\", os.path.exists(os.environ[\"JAVA_HOME\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4a5c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0                         1                             2     3   \\\n",
      "0  movieId                     title                        genres  year   \n",
      "1        1          Toy Story (1995)   Animation|Children's|Comedy  1995   \n",
      "2        2            Jumanji (1995)  Adventure|Children's|Fantasy  1995   \n",
      "3        3   Grumpier Old Men (1995)                Comedy|Romance  1995   \n",
      "4        4  Waiting to Exhale (1995)                  Comedy|Drama  1995   \n",
      "\n",
      "                  4        5   \\\n",
      "0        clean_title  tmdb_id   \n",
      "1          Toy Story    862.0   \n",
      "2            Jumanji   8844.0   \n",
      "3   Grumpier Old Men  15602.0   \n",
      "4  Waiting to Exhale  31357.0   \n",
      "\n",
      "                                                  6   \\\n",
      "0                                           overview   \n",
      "1  Led by Woody, Andy's toys live happily in his ...   \n",
      "2  When siblings Judy and Peter discover an encha...   \n",
      "3  A family wedding reignites the ancient feud be...   \n",
      "4  Cheated on, mistreated and stepped on, the wom...   \n",
      "\n",
      "                                                  7   \\\n",
      "0                                        poster_path   \n",
      "1  https://image.tmdb.org/t/p/w500/uXDfjJbdP4ijW5...   \n",
      "2  https://image.tmdb.org/t/p/w500/p67m5dzwyxWd46...   \n",
      "3  https://image.tmdb.org/t/p/w500/1FSXpj5e8l4KH6...   \n",
      "4  https://image.tmdb.org/t/p/w500/qJU6rfil5xLVb5...   \n",
      "\n",
      "                                                  8             9   \\\n",
      "0                                      backdrop_path  vote_average   \n",
      "1  https://image.tmdb.org/t/p/w500/3Rfvhy1Nl6sSGJ...           8.0   \n",
      "2  https://image.tmdb.org/t/p/w500/pYw10zrqfkdm3y...         7.238   \n",
      "3  https://image.tmdb.org/t/p/w500/1o4vuCHpmd4DXo...         6.461   \n",
      "4  https://image.tmdb.org/t/p/w500/jZjoEKXMTDoZAG...           6.3   \n",
      "\n",
      "           10                                    11  \\\n",
      "0  vote_count                           tmdb_genres   \n",
      "1     18939.0  Animation, Adventure, Family, Comedy   \n",
      "2     10806.0            Adventure, Fantasy, Family   \n",
      "3       399.0                       Romance, Comedy   \n",
      "4       173.0                Comedy, Drama, Romance   \n",
      "\n",
      "                                                12               13  \\\n",
      "0                                       top_3_cast        directors   \n",
      "1                Tom Hanks, Tim Allen, Don Rickles    John Lasseter   \n",
      "2    Robin Williams, Kirsten Dunst, Bradley Pierce     Joe Johnston   \n",
      "3         Walter Matthau, Jack Lemmon, Ann-Margret    Howard Deutch   \n",
      "4  Whitney Houston, Angela Bassett, Loretta Devine  Forest Whitaker   \n",
      "\n",
      "                                                  14  \\\n",
      "0                                           keywords   \n",
      "1  rescue, friendship, mission, jealousy, villain...   \n",
      "2  giant insect, board game, disappearance, jungl...   \n",
      "3  fishing, sequel, old man, best friend, wedding...   \n",
      "4  based on novel or book, single mother, divorce...   \n",
      "\n",
      "                                            15  \n",
      "0                                 trailer_link  \n",
      "1  https://www.youtube.com/watch?v=CxwTLktovTU  \n",
      "2  https://www.youtube.com/watch?v=veszTagaXik  \n",
      "3  https://www.youtube.com/watch?v=rEnOoWs3FuA  \n",
      "4  https://www.youtube.com/watch?v=j9xml1CxgXI  \n",
      "Ratings:\n",
      "   userId  movieId  rating  timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "\n",
      "Users:\n",
      "   userId gender  age  occupation    zip\n",
      "0       1      F    1          10  48067\n",
      "1       2      M   56          16  70072\n",
      "2       3      M   25          15  55117\n",
      "3       4      M   45           7  02460\n",
      "4       5      M   25          20  55455\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\pricc\\Downloads\\movies_enriched_full.csv\"\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "movie_df = (\"movies_enriched_full.csv\")\n",
    "\n",
    "# File paths\n",
    "ratings_path = r\"C:\\Users\\pricc\\Downloads\\ratings.dat\"\n",
    "users_path = r\"C:\\Users\\pricc\\Downloads\\users.dat\"\n",
    "\n",
    "# Load ratings.dat\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Load users.dat\n",
    "users = pd.read_csv(\n",
    "    users_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    ")\n",
    "\n",
    "# Preview the data\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37b0ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models built and similarity matrices saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load Enriched Movie Data\n",
    "# ----------------------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\pricc\\Downloads\\movies_enriched_full.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# SCombine Metadata Fields\n",
    "# ----------------------------------------\n",
    "\n",
    "def combine_metadata(row):\n",
    "    return \" \".join([\n",
    "        str(row[\"tmdb_genres\"]) if pd.notnull(row[\"tmdb_genres\"]) else \"\",\n",
    "        str(row[\"keywords\"]) if pd.notnull(row[\"keywords\"]) else \"\",\n",
    "        str(row[\"top_3_cast\"]) if pd.notnull(row[\"top_3_cast\"]) else \"\",\n",
    "        str(row[\"directors\"]) if pd.notnull(row[\"directors\"]) else \"\"\n",
    "    ]).lower().replace(\",\", \" \").replace(\":\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "df[\"metadata\"] = df.apply(combine_metadata, axis=1)\n",
    "\n",
    "# ----------------------------------------\n",
    "#  Build Vectorizers\n",
    "# ----------------------------------------\n",
    "\n",
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(df[\"metadata\"])\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"metadata\"])\n",
    "\n",
    "# ----------------------------------------\n",
    "#  Compute Cosine Similarity\n",
    "# ----------------------------------------\n",
    "\n",
    "cosine_sim_count = cosine_similarity(count_matrix)\n",
    "cosine_sim_tfidf = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# ----------------------------------------\n",
    "#  Save Results\n",
    "# ----------------------------------------\n",
    "\n",
    "# Save similarity matrices as NumPy arrays\n",
    "np.save(\"cosine_sim_count.npy\", cosine_sim_count)\n",
    "np.save(\"cosine_sim_tfidf.npy\", cosine_sim_tfidf)\n",
    "\n",
    "# Optional: Save similarity matrices as CSVs\n",
    "pd.DataFrame(cosine_sim_count, index=df[\"title\"], columns=df[\"title\"]).to_csv(\"cosine_sim_count.csv\")\n",
    "pd.DataFrame(cosine_sim_tfidf, index=df[\"title\"], columns=df[\"title\"]).to_csv(\"cosine_sim_tfidf.csv\")\n",
    "\n",
    "print(\" Models built and similarity matrices saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f81fd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39a43e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 similar movies to Toy story using TF-IDF:\n",
      "Small Soldiers (1998) (Score: 0.3902)\n",
      "Toy Story 2 (1999) (Score: 0.3538)\n",
      "Indian in the Cupboard, The (1995) (Score: 0.3100)\n",
      "Toys (1992) (Score: 0.2657)\n",
      "Babes in Toyland (1961) (Score: 0.2394)\n"
     ]
    }
   ],
   "source": [
    "def recommend_movies(title, similarity_matrix, df, top_n=10):\n",
    "    idx = df[df[\"title\"] == title].index[0]\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    similar_movies = [(df.iloc[i][\"title\"], score) for i, score in sim_scores]\n",
    "    return similar_movies\n",
    "\n",
    "# Example: Recommend movies similar to \"Toy Story (1995)\"\n",
    "print(\"\\nTop 5 similar movies to Toy story using TF-IDF:\")\n",
    "for movie, score in recommend_movies(\"Toy Story (1995)\", cosine_sim_tfidf, df, top_n=5):\n",
    "    print(f\"{movie} (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba30e47",
   "metadata": {},
   "source": [
    " TF-IDF similarity results for movies similar to Toy Story.\n",
    " TF-IDF (Term Frequency–Inverse Document Frequency) compareD the textual content (plot summaries, keywords, etc.) of movies Similar to Toy Story. Based on this:\n",
    "\n",
    "Small Soldiers is the top match at 39%. It features toys coming to life, much like Toy Story, but with a military/sci-fi twist.\n",
    "\n",
    "Toy Story 2 being similar makes perfect sense at 35%. Tt's a direct sequel with near-identical characters and themes.\n",
    "\n",
    "The Indian in the Cupboard at 31%, also shares the theme of toys/figures coming to life and interacting with a child.\n",
    "\n",
    "Toys and Babes in Toyland at 23% carries themes centered around toy worlds or imaginative play, aligning them conceptually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f273ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations(title, similarity_matrix, df, top_n=5):\n",
    "    idx = df[df[\"title\"] == title].index[0]\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    print(f\"\\n Top {top_n} similar movies to '{title}':\")\n",
    "    for i, (movie_idx, score) in enumerate(sim_scores, 1):\n",
    "        print(f\"{i}. {df.iloc[movie_idx]['title']} (Similarity: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41fd0b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CountVectorizer Recommendations\n",
      "\n",
      " Top 5 similar movies to 'Toy Story (1995)':\n",
      "1. Toy Story 2 (1999) (Similarity: 0.4518)\n",
      "2. Small Soldiers (1998) (Similarity: 0.3790)\n",
      "3. Indian in the Cupboard, The (1995) (Similarity: 0.2887)\n",
      "4. Big (1988) (Similarity: 0.2502)\n",
      "5. Babes in Toyland (1961) (Similarity: 0.2485)\n"
     ]
    }
   ],
   "source": [
    "print(' CountVectorizer Recommendations')\n",
    "print_recommendations(\"Toy Story (1995)\", cosine_sim_count, df, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1156b",
   "metadata": {},
   "source": [
    "CountVectorizer similarity results for movies similar to Toy Story.\n",
    "\n",
    "CountVectorizer converts text (e.g., movie plot summaries) into a matrix of token counts.\n",
    "\n",
    "It builds a bag-of-words representation, capturing how often each word appears, but ignores word order and semantic meaning.\n",
    "\n",
    "Similarity is then computed using cosine similarity between these count vectors.\n",
    "\n",
    "According to  CountVectorizer Recommendations the similarity of other movies to Toy Story is very low. The highest recommended movie is Toy story 2 that is 45% similar, Followed by Small soldiers at 38%. CountVectorizer returns a lower similarity at 28% compared to TD-IDF that recommended the same movie at 31%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "566a51af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine Similarity Matrix (CountVectorizer):\n",
      "title                               Toy Story (1995)  Jumanji (1995)  \\\n",
      "title                                                                  \n",
      "Toy Story (1995)                            1.000000        0.054433   \n",
      "Jumanji (1995)                              0.054433        1.000000   \n",
      "Grumpier Old Men (1995)                     0.054433        0.000000   \n",
      "Waiting to Exhale (1995)                    0.066227        0.064889   \n",
      "Father of the Bride Part II (1995)          0.046676        0.034300   \n",
      "\n",
      "title                               Grumpier Old Men (1995)  \\\n",
      "title                                                         \n",
      "Toy Story (1995)                                   0.054433   \n",
      "Jumanji (1995)                                     0.000000   \n",
      "Grumpier Old Men (1995)                            1.000000   \n",
      "Waiting to Exhale (1995)                           0.097333   \n",
      "Father of the Bride Part II (1995)                 0.068599   \n",
      "\n",
      "title                               Waiting to Exhale (1995)  \\\n",
      "title                                                          \n",
      "Toy Story (1995)                                    0.066227   \n",
      "Jumanji (1995)                                      0.064889   \n",
      "Grumpier Old Men (1995)                             0.097333   \n",
      "Waiting to Exhale (1995)                            1.000000   \n",
      "Father of the Bride Part II (1995)                  0.027821   \n",
      "\n",
      "title                               Father of the Bride Part II (1995)  \n",
      "title                                                                   \n",
      "Toy Story (1995)                                              0.046676  \n",
      "Jumanji (1995)                                                0.034300  \n",
      "Grumpier Old Men (1995)                                       0.068599  \n",
      "Waiting to Exhale (1995)                                      0.027821  \n",
      "Father of the Bride Part II (1995)                            1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load movie data and similarity matrices\n",
    "df = pd.read_csv(r\"C:\\Users\\pricc\\Downloads\\movies_enriched_full.csv\")\n",
    "cosine_sim_count = np.load(r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\cosine_sim_count.npy\")\n",
    "cosine_sim_tfidf = np.load(r\"C:\\Users\\pricc\\OneDrive\\Desktop\\pyspark\\cosine_sim_tfidf.npy\")\n",
    "print(\" Cosine Similarity Matrix (CountVectorizer):\")\n",
    "print(pd.DataFrame(cosine_sim_count, index=df[\"title\"], columns=df[\"title\"]).iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6430f",
   "metadata": {},
   "source": [
    " Cosine Similarity Matrix (CountVectorizer)\n",
    "CountVectorizer is Ideal for quick recommendations when using well-structured metadata or synopses.\n",
    "Self-similarity between Toy story and Toy Story is always 1.0, so Toy Story is 100% similar to itself.\n",
    "\n",
    "The similarity between Toy Story and Jumanji is 0.054, indicating very low textual similarity (that could mean in plot summary or keywords don't overlap much).\n",
    "\n",
    "Waiting to Exhale actually has a higher similarity (0.066) to Toy Story than Jumanji does which is counterintuitive but reflects overlap in words/phrases rather than theme or genre.\n",
    "\n",
    "Grumpier Old Men and Father of the Bride Part II also show weak similarity, likely due to shared generic terms like \"family\", \"life\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7cba43",
   "metadata": {},
   "source": [
    "COMPARISON BETWEEN TD-IDF and CountVectorizer\n",
    "CountVectorizer treats plots as just word counts, ignoring word order or semantics.\n",
    "\n",
    "CountVectorizer scores don’t mean thematic or genre similarity,only textual overlap in description/metadata.\n",
    "\n",
    "According to CountVectorizer Toy Story is weakly similar to other 1995 films based.\n",
    "\n",
    "TF-IDF gives more refined similarity (as you saw earlier), while CountVectorizer tends to inflate common word matches.\n",
    "\n",
    "Using TF-IDF + cosine offers deeper semantic similarity.\n",
    "\n",
    "Both have 3 movies in common as highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b5ea6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine Similarity Matrix (CountVectorizer):\n",
      "title                               Toy Story (1995)  Jumanji (1995)  \\\n",
      "title                                                                  \n",
      "Toy Story (1995)                            1.000000        0.054433   \n",
      "Jumanji (1995)                              0.054433        1.000000   \n",
      "Grumpier Old Men (1995)                     0.054433        0.000000   \n",
      "Waiting to Exhale (1995)                    0.066227        0.064889   \n",
      "Father of the Bride Part II (1995)          0.046676        0.034300   \n",
      "\n",
      "title                               Grumpier Old Men (1995)  \\\n",
      "title                                                         \n",
      "Toy Story (1995)                                   0.054433   \n",
      "Jumanji (1995)                                     0.000000   \n",
      "Grumpier Old Men (1995)                            1.000000   \n",
      "Waiting to Exhale (1995)                           0.097333   \n",
      "Father of the Bride Part II (1995)                 0.068599   \n",
      "\n",
      "title                               Waiting to Exhale (1995)  \\\n",
      "title                                                          \n",
      "Toy Story (1995)                                    0.066227   \n",
      "Jumanji (1995)                                      0.064889   \n",
      "Grumpier Old Men (1995)                             0.097333   \n",
      "Waiting to Exhale (1995)                            1.000000   \n",
      "Father of the Bride Part II (1995)                  0.027821   \n",
      "\n",
      "title                               Father of the Bride Part II (1995)  \n",
      "title                                                                   \n",
      "Toy Story (1995)                                              0.046676  \n",
      "Jumanji (1995)                                                0.034300  \n",
      "Grumpier Old Men (1995)                                       0.068599  \n",
      "Waiting to Exhale (1995)                                      0.027821  \n",
      "Father of the Bride Part II (1995)                            1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load movie data and similarity matrices\n",
    "df = pd.read_csv(\"movies_enriched_full.csv\")\n",
    "cosine_sim_count = np.load(\"cosine_sim_count.npy\")\n",
    "cosine_sim_tfidf = np.load(\"cosine_sim_tfidf.npy\")\n",
    "print(\" Cosine Similarity Matrix (CountVectorizer):\")\n",
    "print(pd.DataFrame(cosine_sim_count, index=df[\"title\"], columns=df[\"title\"]).iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972dcb1",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c5a30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cosine Similarity Matrix (TF-IDF):\n",
      "title                               Toy Story (1995)  Jumanji (1995)  \\\n",
      "title                                                                  \n",
      "Toy Story (1995)                            1.000000        0.014150   \n",
      "Jumanji (1995)                              0.014150        1.000000   \n",
      "Grumpier Old Men (1995)                     0.022040        0.000000   \n",
      "Waiting to Exhale (1995)                    0.028133        0.017912   \n",
      "Father of the Bride Part II (1995)          0.009242        0.009244   \n",
      "\n",
      "title                               Grumpier Old Men (1995)  \\\n",
      "title                                                         \n",
      "Toy Story (1995)                                   0.022040   \n",
      "Jumanji (1995)                                     0.000000   \n",
      "Grumpier Old Men (1995)                            1.000000   \n",
      "Waiting to Exhale (1995)                           0.017067   \n",
      "Father of the Bride Part II (1995)                 0.024845   \n",
      "\n",
      "title                               Waiting to Exhale (1995)  \\\n",
      "title                                                          \n",
      "Toy Story (1995)                                    0.028133   \n",
      "Jumanji (1995)                                      0.017912   \n",
      "Grumpier Old Men (1995)                             0.017067   \n",
      "Waiting to Exhale (1995)                            1.000000   \n",
      "Father of the Bride Part II (1995)                  0.003701   \n",
      "\n",
      "title                               Father of the Bride Part II (1995)  \n",
      "title                                                                   \n",
      "Toy Story (1995)                                              0.009242  \n",
      "Jumanji (1995)                                                0.009244  \n",
      "Grumpier Old Men (1995)                                       0.024845  \n",
      "Waiting to Exhale (1995)                                      0.003701  \n",
      "Father of the Bride Part II (1995)                            1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Cosine Similarity Matrix (TF-IDF):\")\n",
    "print(pd.DataFrame(cosine_sim_tfidf, index=df[\"title\"], columns=df[\"title\"]).iloc[:5, :5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f748719",
   "metadata": {},
   "source": [
    "````````````````````````````````````````````````````````````````````````````````````````\n",
    "\n",
    "Memory-Based Collaborative Filtering (Bias-Normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfb37b",
   "metadata": {},
   "source": [
    "User-Based CF Recommendations for User 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "614a48d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 50 User-Based CF Recommendations for User 48:\n",
      "   movieId                                              title     score\n",
      "0      750  Dr. Strangelove or: How I Learned to Stop Worr...  3.818245\n",
      "1      904                                 Rear Window (1954)  3.756734\n",
      "2      908                          North by Northwest (1959)  3.723104\n",
      "3     1089                              Reservoir Dogs (1992)  3.708471\n",
      "4     1234                                  Sting, The (1973)  3.702266\n",
      "5      953                       It's a Wonderful Life (1946)  3.700303\n",
      "6     1252                                   Chinatown (1974)  3.675555\n",
      "7     1278                          Young Frankenstein (1974)  3.674163\n",
      "8     1225                                     Amadeus (1984)  3.668453\n",
      "9     2997                        Being John Malkovich (1999)  3.665072\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "\n",
    "# ==============================\n",
    "# Load and Subset Data\n",
    "# ==============================\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(file_path)[['movieId', 'title']]\n",
    "\n",
    "# Use smaller subset\n",
    "subset_users = ratings['userId'].value_counts().head(500).index\n",
    "subset_movies = ratings['movieId'].value_counts().head(500).index\n",
    "ratings_small = ratings[ratings['userId'].isin(subset_users) & ratings['movieId'].isin(subset_movies)]\n",
    "\n",
    "# ==============================\n",
    "# Create Bias-Normalized Matrix\n",
    "# ==============================\n",
    "\n",
    "def create_normalized_user_item_matrix(ratings):\n",
    "    matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_means = matrix.mean(axis=1)\n",
    "    return matrix.sub(user_means, axis=0).fillna(0), user_means\n",
    "\n",
    "user_item_matrix, user_means = create_normalized_user_item_matrix(ratings_small)\n",
    "\n",
    "# ==============================\n",
    "# Compute User Similarity Matrix\n",
    "# ==============================\n",
    "\n",
    "def compute_similarity(matrix, kind='user'):\n",
    "    if kind == 'user':\n",
    "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
    "    else:\n",
    "        raise ValueError(\"Only 'user' similarity supported in this run\")\n",
    "    return sim\n",
    "\n",
    "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
    "\n",
    "# ==============================\n",
    "# User-Based Recommendation Function\n",
    "# ==============================\n",
    "\n",
    "def recommend_user_based(user_id, user_item_matrix, user_means, similarity_matrix, top_n=50):\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        print(f\"User {user_id} not found in data.\")\n",
    "        return pd.DataFrame(columns=['movieId', 'score'])\n",
    "\n",
    "    user_sim_scores = similarity_matrix[user_item_matrix.index.get_loc(user_id)]\n",
    "    normalized_ratings = user_item_matrix.values\n",
    "\n",
    "    weighted_scores = user_sim_scores @ normalized_ratings\n",
    "    sum_weights = np.abs(user_sim_scores).sum()\n",
    "\n",
    "    if sum_weights == 0:\n",
    "        return pd.DataFrame(columns=['movieId', 'score'])\n",
    "\n",
    "    predicted_ratings = weighted_scores / sum_weights\n",
    "    predicted_ratings += user_means.loc[user_id]\n",
    "\n",
    "    user_seen = user_item_matrix.loc[user_id]\n",
    "    unseen_mask = user_seen == 0\n",
    "    recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\\\n",
    "        .sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'movieId': recs.index,\n",
    "        'score': recs.values\n",
    "    })\n",
    "\n",
    "# ==============================\n",
    "# Get Top-50 Recommendations for User 48\n",
    "# ==============================\n",
    "\n",
    "test_user_id = 48\n",
    "\n",
    "user_cf_recs = recommend_user_based(\n",
    "    test_user_id,\n",
    "    user_item_matrix,\n",
    "    user_means,\n",
    "    user_sim_matrix,\n",
    "    top_n=50\n",
    ")\n",
    "\n",
    "user_cf_recs = user_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
    "\n",
    "# ==============================\n",
    "# Display Recommendations\n",
    "# ==============================\n",
    "\n",
    "print(f\"\\n Top 50 User-Based CF Recommendations for User {test_user_id}:\")\n",
    "print(user_cf_recs[['movieId', 'title', 'score']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6546a19",
   "metadata": {},
   "source": [
    "Item-Based CF Recommendations for user 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b68c5942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based similarity computed. Shape: (500, 500)\n",
      "\n",
      " Top 50 Item-Based CF Recommendations (Raw Matrix) for User 48:\n",
      "   movieId                             title     score\n",
      "0     1952            Midnight Cowboy (1969)  3.414641\n",
      "1     1244                  Manhattan (1979)  3.412156\n",
      "2     1084           Bonnie and Clyde (1967)  3.409158\n",
      "3     1230                 Annie Hall (1977)  3.409011\n",
      "4     1252                  Chinatown (1974)  3.408365\n",
      "5     1172            Cinema Paradiso (1988)  3.407679\n",
      "6     1957           Chariots of Fire (1981)  3.407620\n",
      "7     1228                Raging Bull (1980)  3.407214\n",
      "8     1267  Manchurian Candidate, The (1962)  3.406944\n",
      "9      908         North by Northwest (1959)  3.406765\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "\n",
    "# ==============================\n",
    "# Load and Subset Data\n",
    "# ==============================\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(file_path)[['movieId', 'title']]\n",
    "\n",
    "# Use smaller subset\n",
    "subset_users = ratings['userId'].value_counts().head(500).index\n",
    "subset_movies = ratings['movieId'].value_counts().head(500).index\n",
    "ratings_small = ratings[ratings['userId'].isin(subset_users) & ratings['movieId'].isin(subset_movies)]\n",
    "\n",
    "# ==============================\n",
    "# Create Normalized & Raw User-Item Matrices\n",
    "# ==============================\n",
    "\n",
    "def create_normalized_user_item_matrix(ratings):\n",
    "    matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_means = matrix.mean(axis=1)\n",
    "    return matrix.sub(user_means, axis=0).fillna(0), user_means\n",
    "\n",
    "user_item_matrix_norm, user_means = create_normalized_user_item_matrix(ratings_small)\n",
    "user_item_matrix_raw = ratings_small.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# Compute Similarity (Raw for Item-CF)\n",
    "# ==============================\n",
    "\n",
    "def compute_similarity(matrix, kind='user'):\n",
    "    if kind == 'user':\n",
    "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
    "    elif kind == 'item':\n",
    "        sim = 1 - pairwise_distances(matrix.T, metric='cosine')\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
    "    print(f\"{kind.title()}-based similarity computed. Shape: {sim.shape}\")\n",
    "    return sim\n",
    "\n",
    "item_sim_matrix_raw = compute_similarity(user_item_matrix_raw, kind='item')\n",
    "\n",
    "# ==============================\n",
    "# Recommendation Function (Item-Based, Raw)\n",
    "# ==============================\n",
    "\n",
    "def recommend_memory_based(user_id, user_item_matrix, user_means, similarity_matrix, kind='item', top_n=50):\n",
    "    model_label = f\"{kind.title()}-Based CF\"\n",
    "\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        print(f\"User {user_id} not in matrix.\")\n",
    "        return pd.DataFrame(columns=['movieId', 'score', 'model'])\n",
    "\n",
    "    if kind == 'item':\n",
    "        user_ratings = user_item_matrix.loc[user_id]\n",
    "        scores = user_ratings @ similarity_matrix\n",
    "        sum_weights = (user_ratings != 0) @ np.abs(similarity_matrix)\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            predicted_ratings = np.true_divide(scores, sum_weights)\n",
    "            predicted_ratings[sum_weights == 0] = 0\n",
    "\n",
    "        unseen_mask = user_ratings == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\\\n",
    "            .sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Only item-based CF supported in this call\") \n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'movieId': recs.index,\n",
    "        'score': recs.values,\n",
    "        'model': model_label\n",
    "    })\n",
    "\n",
    "# ==============================\n",
    "# Generate Top-50 Item-CF Recs for User 48\n",
    "# ==============================\n",
    "\n",
    "test_user_id = 48\n",
    "\n",
    "item_cf_recs_user48 = recommend_memory_based(\n",
    "    test_user_id,\n",
    "    user_item_matrix_raw,\n",
    "    None,\n",
    "    item_sim_matrix_raw,\n",
    "    kind='item',\n",
    "    top_n=50\n",
    ")\n",
    "\n",
    "item_cf_recs_user48 = item_cf_recs_user48.merge(movies, on=\"movieId\", how=\"left\")\n",
    "\n",
    "# ==============================\n",
    "# Display\n",
    "# ==============================\n",
    "\n",
    "print(f\"\\n Top 50 Item-Based CF Recommendations (Raw Matrix) for User {test_user_id}:\")\n",
    "print(item_cf_recs_user48[['movieId', 'title', 'score']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291b932",
   "metadata": {},
   "source": [
    "COMPARISON OF User based CF recommendations and Item-Based CF Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a7ac1",
   "metadata": {},
   "source": [
    "The User based CF recommendation score is the predicted rating for an unseen movie, based on users similar to User 48 ratings, adjusted to the target user’s scale. Weighted average of normalized ratings from similar users was re-centered to user. UBCF recommended movies with Higher scores that User 48 is more strongly predicted to like. How i learned to stop to worry was most highly rated at 3.8 score.\n",
    "\n",
    "The Top 50 Item-Based CF Recommendations for User 48 score above is the predicted rating for an unseen movie, based on how similar it is to the movies the user 48 has already rated. Weighted average of user’s ratings for similar items was used. Compared to UBCF, Item based recommended movies have Lower scores Midnight Cowboy has a predicated rating of 3.4 meaning User 48 is predicted to like Item based recommendations less.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4efcdd",
   "metadata": {},
   "source": [
    "`````````````````````````````````````````````````````````````````````````````````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd765a18",
   "metadata": {},
   "source": [
    "`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4028ad6",
   "metadata": {},
   "source": [
    "User Based CF RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f25ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28765/28765 [00:11<00:00, 2453.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (User-Based CF): 0.8589\n",
      "Precision@10: 0.8492\n",
      "Recall@10:    0.2670\n",
      "NDCG@10:      0.8652\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Create normalized user-item matrix (mean-centered)\n",
    "# ----------------------------\n",
    "def create_normalized_user_item_matrix(ratings_df):\n",
    "    matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_means = matrix.mean(axis=1)\n",
    "    matrix_centered = matrix.sub(user_means, axis=0)\n",
    "    return matrix_centered, user_means\n",
    "\n",
    "train_matrix_norm, train_user_means = create_normalized_user_item_matrix(train_ratings)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute user-user cosine similarity\n",
    "# ----------------------------\n",
    "def compute_user_similarity(norm_matrix):\n",
    "    sim = cosine_similarity(norm_matrix.fillna(0))\n",
    "    return pd.DataFrame(sim, index=norm_matrix.index, columns=norm_matrix.index)\n",
    "\n",
    "user_sim_matrix = compute_user_similarity(train_matrix_norm)\n",
    "\n",
    "# ----------------------------\n",
    "# Predict using User-Based CF with normalized matrix\n",
    "# ----------------------------\n",
    "def predict_user_based(test_df, norm_matrix, user_means, user_sim):\n",
    "    preds = []\n",
    "\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        user, item = row['userId'], row['movieId']\n",
    "        if user not in norm_matrix.index or item not in norm_matrix.columns:\n",
    "            preds.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        sims = user_sim[user]\n",
    "        item_ratings = norm_matrix[item]\n",
    "\n",
    "        mask = item_ratings.notna()\n",
    "        sims = sims[mask]\n",
    "        item_ratings = item_ratings[mask]\n",
    "\n",
    "        if len(sims) == 0:\n",
    "            preds.append(np.nan)\n",
    "        else:\n",
    "            weighted_sum = np.dot(sims, item_ratings)\n",
    "            sum_weights = np.sum(np.abs(sims)) + 1e-8\n",
    "            pred = train_user_means[user] + weighted_sum / sum_weights\n",
    "            preds.append(pred)\n",
    "\n",
    "    result = test_df.copy()\n",
    "    result['predicted'] = preds\n",
    "    return result\n",
    "\n",
    "user_preds = predict_user_based(test_ratings, train_matrix_norm, train_user_means, user_sim_matrix)\n",
    "\n",
    "# ----------------------------\n",
    "# Clip and clean predictions\n",
    "# ----------------------------\n",
    "user_preds['predicted'] = user_preds['predicted'].clip(1, 5)\n",
    "user_preds_valid = user_preds.dropna()\n",
    "user_preds_valid = user_preds_valid[user_preds_valid['predicted'] > 0]\n",
    "\n",
    "# ----------------------------\n",
    "# RMSE\n",
    "# ----------------------------\n",
    "def compute_rmse_from_df(pred_df):\n",
    "    return sqrt(mean_squared_error(pred_df['rating'], pred_df['predicted']))\n",
    "\n",
    "rmse_user = compute_rmse_from_df(user_preds_valid)\n",
    "print(f\"Test RMSE (User-Based CF): {rmse_user:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Precision@K, Recall@K, NDCG@K\n",
    "# ----------------------------\n",
    "def precision_recall_ndcg_at_k(pred_df, truth_df, k=10, threshold=4.0):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for user_id in pred_df.index:\n",
    "        if user_id not in truth_df.index:\n",
    "            continue\n",
    "\n",
    "        pred_ratings = pred_df.loc[user_id].dropna().sort_values(ascending=False)\n",
    "        true_ratings = truth_df.loc[user_id]\n",
    "\n",
    "        top_k_items = pred_ratings.head(k).index\n",
    "        relevant_items = true_ratings[true_ratings >= threshold].index\n",
    "\n",
    "        if len(relevant_items) == 0:\n",
    "            continue\n",
    "\n",
    "        hits = [1 if item in relevant_items else 0 for item in top_k_items]\n",
    "\n",
    "        precision = np.sum(hits) / k\n",
    "        recall = np.sum(hits) / len(relevant_items)\n",
    "        dcg = np.sum([hit / np.log2(idx + 2) for idx, hit in enumerate(hits)])\n",
    "        idcg = np.sum([1 / np.log2(i + 2) for i in range(min(len(relevant_items), k))])\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)\n",
    "\n",
    "# Create user-item prediction matrix\n",
    "pred_matrix = user_preds_valid.pivot(index='userId', columns='movieId', values='predicted')\n",
    "truth_matrix = test_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Evaluate\n",
    "p, r, n = precision_recall_ndcg_at_k(pred_matrix, truth_matrix, k=10)\n",
    "print(f\"Precision@10: {p:.4f}\")\n",
    "print(f\"Recall@10:    {r:.4f}\")\n",
    "print(f\"NDCG@10:      {n:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f64d75",
   "metadata": {},
   "source": [
    "The UBCF RMSE 0.8589 is Very good, it indicates the predicted ratings are close to actual ratings. It is lower than IBCF RMSE 0.967\n",
    "Precision@10 at 0.8492\tis Excellent because 84% of the top-10 recommendations are relevant (liked by the user).\n",
    "Recall@10 of 0.2670\tis decent because 26% of all relevant items are retrieved in the top-10. Low recall is common in sparse data.\n",
    "NDCG@10\t0.8652 is very strong, the relevant items are ranked high in the list, indicating great ordering of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792eec2",
   "metadata": {},
   "source": [
    "``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430e284",
   "metadata": {},
   "source": [
    "Item-Based CF RMSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afdeda57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28765/28765 [00:12<00:00, 2397.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (Item-Based CF): 0.9676\n",
      "Precision@10: 0.7942\n",
      "Recall@10:    0.2452\n",
      "NDCG@10:      0.8013\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Build raw user-item matrix\n",
    "# ----------------------------\n",
    "train_matrix_raw = train_ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "\n",
    "# ----------------------------\n",
    "#  Compute item-item cosine similarity\n",
    "# ----------------------------\n",
    "def compute_similarity(matrix, kind='item'):\n",
    "    if kind == 'item':\n",
    "        sim = cosine_similarity(matrix.T.fillna(0))\n",
    "        return pd.DataFrame(sim, index=matrix.columns, columns=matrix.columns)\n",
    "    elif kind == 'user':\n",
    "        sim = cosine_similarity(matrix.fillna(0))\n",
    "        return pd.DataFrame(sim, index=matrix.index, columns=matrix.index)\n",
    "\n",
    "item_sim_matrix = compute_similarity(train_matrix_raw, kind='item')\n",
    "\n",
    "# ----------------------------\n",
    "# Predict ratings using item-based CF\n",
    "# ----------------------------\n",
    "def predict_item_based(test_df, train_matrix, item_sim):\n",
    "    preds = []\n",
    "\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        user, item = row['userId'], row['movieId']\n",
    "        if item not in item_sim.index or user not in train_matrix.index:\n",
    "            preds.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        sims = item_sim[item]\n",
    "        user_ratings = train_matrix.loc[user]\n",
    "        mask = user_ratings > 0\n",
    "        sims = sims[mask.index[mask]]\n",
    "        user_ratings = user_ratings[mask]\n",
    "\n",
    "        if len(sims) == 0:\n",
    "            preds.append(np.nan)\n",
    "        else:\n",
    "            numerator = np.dot(sims.values, user_ratings.values)\n",
    "            denominator = np.sum(np.abs(sims.values)) + 1e-8\n",
    "            preds.append(numerator / denominator)\n",
    "\n",
    "    result = test_df.copy()\n",
    "    result['predicted'] = preds\n",
    "    return result\n",
    "\n",
    "item_preds = predict_item_based(test_ratings, train_matrix_raw, item_sim_matrix)\n",
    "\n",
    "# ----------------------------\n",
    "#  Clean predictions and clip to [1, 5]\n",
    "# ----------------------------\n",
    "item_preds['predicted'] = item_preds['predicted'].clip(1, 5)\n",
    "item_preds_valid = item_preds.dropna()\n",
    "item_preds_valid = item_preds_valid[item_preds_valid['predicted'] > 0]\n",
    "\n",
    "# ----------------------------\n",
    "#  RMSE\n",
    "# ----------------------------\n",
    "def compute_rmse_from_df(pred_df):\n",
    "    return sqrt(mean_squared_error(pred_df['rating'], pred_df['predicted']))\n",
    "\n",
    "rmse_item = compute_rmse_from_df(item_preds_valid)\n",
    "print(f\"Test RMSE (Item-Based CF): {rmse_item:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate Top-K Metrics (Precision@K, Recall@K, NDCG@K)\n",
    "# ----------------------------\n",
    "# Pivot predictions and truth to user-item matrices\n",
    "pred_matrix = item_preds_valid.pivot(index='userId', columns='movieId', values='predicted')\n",
    "truth_matrix = test_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "def precision_recall_ndcg_at_k(pred_df, truth_df, k=10, threshold=4.0):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for user_id in pred_df.index:\n",
    "        if user_id not in truth_df.index:\n",
    "            continue\n",
    "\n",
    "        pred_ratings = pred_df.loc[user_id].dropna().sort_values(ascending=False)\n",
    "        true_ratings = truth_df.loc[user_id]\n",
    "\n",
    "        top_k_items = pred_ratings.head(k).index\n",
    "        relevant_items = true_ratings[true_ratings >= threshold].index\n",
    "\n",
    "        if len(relevant_items) == 0:\n",
    "            continue\n",
    "\n",
    "        hits = [1 if item in relevant_items else 0 for item in top_k_items]\n",
    "\n",
    "        precision = np.sum(hits) / k\n",
    "        recall = np.sum(hits) / len(relevant_items)\n",
    "        dcg = np.sum([hit / np.log2(idx + 2) for idx, hit in enumerate(hits)])\n",
    "        idcg = np.sum([1 / np.log2(i + 2) for i in range(min(len(relevant_items), k))])\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)\n",
    "\n",
    "# Evaluate\n",
    "p, r, n = precision_recall_ndcg_at_k(pred_matrix, truth_matrix, k=10)\n",
    "print(f\"Precision@10: {p:.4f}\")\n",
    "print(f\"Recall@10:    {r:.4f}\")\n",
    "print(f\"NDCG@10:      {n:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca7147",
   "metadata": {},
   "source": [
    "```````````````````````````````````````````````````````````````````````````````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ece61",
   "metadata": {},
   "source": [
    "The IBCF RMSE 0.9676 is Very good, it indicates the predicted ratings are close to actual ratings (low error).\n",
    "Precision@10 at 0.7942\tis Excellent because 79% of the top-10 recommendations are relevant (liked by the user).\n",
    "Recall@10 of 0.2452\tis decent because 24% of all relevant items are retrieved in the top-10. Low recall is common in sparse data.\n",
    "NDCG@10\t0.8013 is very strong, the relevant items are ranked high in the list, indicating great ordering of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba5247",
   "metadata": {},
   "source": [
    "COMPARISON BETWEEN UBCF AND IBCF\n",
    "User-Based CF outperforms Item-Based CF across all metrics in this case.\n",
    "UBCF has Lower RMSE hence more accurate predictions.\n",
    "\n",
    "Higher Precision and NDCG meaning more relevant and better-ordered top recommendations.\n",
    "\n",
    "Higher Recall that translates to more comprehensive retrieval of liked items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bb8dd",
   "metadata": {},
   "source": [
    "```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fe2e5",
   "metadata": {},
   "source": [
    "ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e8b41b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model RMSE: 0.8363\n",
      "+-------+----------------------------+----------+\n",
      "|movieId|title                       |prediction|\n",
      "+-------+----------------------------+----------+\n",
      "|2129   |Saltmen of Tibet, The (1997)|5.052246  |\n",
      "|572    |Foreign Student (1994)      |4.9640427 |\n",
      "|2197   |Firelight (1997)            |4.8641205 |\n",
      "|3382   |Song of Freedom (1936)      |4.8395925 |\n",
      "|1471   |Boys Life 2 (1997)          |4.6993504 |\n",
      "|811    |Bewegte Mann, Der (1994)    |4.578691  |\n",
      "|3365   |Searchers, The (1956)       |4.3095155 |\n",
      "|912    |Casablanca (1942)           |4.302957  |\n",
      "|3849   |Spiral Staircase, The (1946)|4.294216  |\n",
      "|3469   |Inherit the Wind (1960)     |4.2933974 |\n",
      "+-------+----------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# ============================\n",
    "# Initialize Spark Session\n",
    "# ============================\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ALSModel\").getOrCreate()\n",
    "\n",
    "# ============================\n",
    "# Load ratings data as Spark DataFrame\n",
    "# ============================\n",
    "ratings_path = r\"C:\\Users\\pricc\\Downloads\\ratings.dat\"\n",
    "\n",
    "# Since ratings.dat uses '::' as separator, read as text and split manually\n",
    "ratings_raw = spark.read.text(ratings_path)\n",
    "ratings_split = ratings_raw.selectExpr(\n",
    "    \"split(value, '::') as parts\"\n",
    ").select(\n",
    "    col(\"parts\").getItem(0).cast(\"int\").alias(\"userId\"),\n",
    "    col(\"parts\").getItem(1).cast(\"int\").alias(\"movieId\"),\n",
    "    col(\"parts\").getItem(2).cast(\"float\").alias(\"rating\"),\n",
    "    col(\"parts\").getItem(3).cast(\"long\").alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "ratings_split.cache()\n",
    "\n",
    "# ============================\n",
    "# Train ALS model\n",
    "# ============================\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=10,\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "als_model = als.fit(ratings_split)\n",
    "\n",
    "# ============================\n",
    "# Evaluate model RMSE\n",
    "# ============================\n",
    "predictions = als_model.transform(ratings_split)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"ALS model RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# Generate Top-50 Recommendations for User 5549\n",
    "# ============================\n",
    "target_user = 5549\n",
    "\n",
    "# Get all movie IDs\n",
    "all_movie_ids = ratings_split.select(\"movieId\").distinct()\n",
    "\n",
    "# Get movie IDs the user has rated\n",
    "rated_movie_ids = ratings_split.filter(col(\"userId\") == target_user).select(\"movieId\").distinct()\n",
    "\n",
    "# Get unrated movies for this user\n",
    "unrated_movies = all_movie_ids.join(rated_movie_ids, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "# Create dataframe of (userId, movieId) pairs for prediction\n",
    "user_unrated_pairs = unrated_movies.withColumn(\"userId\", col(\"movieId\") * 0 + target_user).select(\"userId\", \"movieId\")\n",
    "\n",
    "# Predict ratings for unrated movies\n",
    "recommendations = als_model.transform(user_unrated_pairs).dropna()\n",
    "\n",
    "# Select top 50 predictions\n",
    "top_50_recs = recommendations.orderBy(col(\"prediction\").desc()).limit(50)\n",
    "\n",
    "# ============================\n",
    "# Load movie titles\n",
    "# ============================\n",
    "movies_path = r\"C:\\Users\\pricc\\Downloads\\movies_enriched_full.csv\"\n",
    "\n",
    "movies_df = spark.read.option(\"header\", \"true\").csv(movies_path).select(\n",
    "    col(\"movieId\").cast(\"int\"),\n",
    "    col(\"title\")\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Join recommendations with movie titles\n",
    "# ============================\n",
    "top_50_with_titles = top_50_recs.join(movies_df, on=\"movieId\", how=\"left\")\n",
    "\n",
    "# ============================\n",
    "# Show Top 10 recommendations\n",
    "# ============================\n",
    "top_50_with_titles.select(\"movieId\", \"title\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# ============================\n",
    "# Stop Spark Session (optional)\n",
    "# ============================\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc0cc8b",
   "metadata": {},
   "source": [
    "ALS has the lowest RMSE, meaning it's the most accurate model for predicting ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc96071",
   "metadata": {},
   "source": [
    "`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060a6c7",
   "metadata": {},
   "source": [
    " Diversity, Novelty, Serendipity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b20af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.sparse import csr_matrix \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommended movie indices for user 50: [   0 2898  309   33  581  574  513  346 2162 2557]\n",
      "Novelty: 0.4773\n",
      "Diversity: 0.6112\n",
      "Serendipity: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Rating matrix: rows are users, columns are movies\n",
    "\n",
    "ratings_matrix = user_item_matrix.copy()  # already centered or filled\n",
    "ratings_filled = ratings_matrix.fillna(0).to_numpy()\n",
    "\n",
    "# Apply SVD\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(ratings_filled)\n",
    "item_factors = svd.components_.T  # shape: (num_movies, 20)\n",
    "\n",
    "# Choose a user to evaluate\n",
    "user_idx = 50\n",
    "user_vector = user_factors[user_idx]\n",
    "\n",
    "# Score all movies for this user\n",
    "scores = np.dot(item_factors, user_vector)\n",
    "\n",
    "# Get top N recommended movies\n",
    "top_n = 10\n",
    "top_items = np.argsort(-scores)[:top_n]\n",
    "\n",
    "# --- Novelty: penalize popular movies ---\n",
    "movie_popularity = np.sum(ratings_filled > 0, axis=0)\n",
    "movie_popularity = movie_popularity / movie_popularity.max()\n",
    "novelty = np.mean([1 - movie_popularity[i] for i in top_items])\n",
    "\n",
    "# --- Diversity: average dissimilarity among recommendations ---\n",
    "item_vecs = item_factors[top_items]\n",
    "sim_matrix = cosine_similarity(item_vecs)\n",
    "upper_triangle = sim_matrix[np.triu_indices(len(top_items), k=1)]\n",
    "diversity = 1 - np.mean(upper_triangle)\n",
    "\n",
    "# --- Serendipity: dissimilar from previously liked movies ---\n",
    "user_ratings = ratings_filled[user_idx]\n",
    "liked_items = np.where(user_ratings >= 4)[0]\n",
    "liked_vecs = item_factors[liked_items]\n",
    "\n",
    "serendipity_scores = []\n",
    "for i in top_items:\n",
    "    rec_vec = item_factors[i].reshape(1, -1)\n",
    "    if liked_vecs.shape[0] > 0:\n",
    "        sim = cosine_similarity(rec_vec, liked_vecs)\n",
    "        serendipity_scores.append(1 - np.mean(sim))\n",
    "\n",
    "serendipity = np.mean(serendipity_scores)\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Top recommended movie indices for user {user_idx}: {top_items}\")\n",
    "print(f\"Novelty: {novelty:.4f}\")\n",
    "print(f\"Diversity: {diversity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f9e3a",
   "metadata": {},
   "source": [
    "````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
    "Novelty,Diversity,serendipity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b838f08",
   "metadata": {},
   "source": [
    "In this first approach, serendipity was just computed by penalizing all high-similarity items,even if some surprise could’ve been good. All recommendations were similar to what the user already liked, I got 0 serendipity. np.mean(similarity_to_liked_items)\n",
    "\n",
    "Novelty of 0.3871 for the top-10 recommendations for each user include items that aren’t too popular.\n",
    "\n",
    "Since the score is close to 0.4, it means not just  blockbusters were recommended, but includes moderately obscure items.\n",
    "\n",
    "A perfect novelty (closer to 1.0) would mean mostly unknown or unrated items.\n",
    "\n",
    "A Diversity of 0.7006 is strong diversity, the recommended items for each user are not overly similar to each other.\n",
    "\n",
    "Diversity above 0.7 is generally considered good, especially for top-10 lists.\n",
    "\n",
    "It means users are seeing a range of genres, styles, or content types rather than a narrow cluster of similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc4a351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|██████████| 6040/6040 [00:07<00:00, 772.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average over all users:\n",
      "Novelty: 0.3871\n",
      "Diversity: 0.7006\n",
      "Serendipity: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # optional: for progress bar\n",
    "\n",
    "# Prepare matrix: users x movies (fill NaN with 0 for SVD)\n",
    "ratings_filled = user_item_matrix.fillna(0).to_numpy()\n",
    "user_ids = user_item_matrix.index\n",
    "movie_ids = user_item_matrix.columns.to_numpy()\n",
    "\n",
    "# Train SVD model\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(ratings_filled)\n",
    "item_factors = svd.components_.T  # shape: (num_movies, 20)\n",
    "\n",
    "# Popularity vector: how often each movie is rated\n",
    "movie_popularity = np.sum(ratings_filled > 0, axis=0)\n",
    "movie_popularity = movie_popularity / movie_popularity.max()\n",
    "\n",
    "# Initialize metric lists\n",
    "novelties = []\n",
    "diversities = []\n",
    "serendipities = []\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for user_idx in tqdm(range(len(user_ids)), desc=\"Evaluating users\"):\n",
    "    user_vector = user_factors[user_idx]\n",
    "    scores = np.dot(item_factors, user_vector)\n",
    "    top_items = np.argsort(-scores)[:top_n]\n",
    "\n",
    "    # Novelty\n",
    "    novelty = np.mean([1 - movie_popularity[i] for i in top_items])\n",
    "    novelties.append(novelty)\n",
    "\n",
    "    # Diversity\n",
    "    item_vecs = item_factors[top_items]\n",
    "    sim_matrix = cosine_similarity(item_vecs)\n",
    "    upper_triangle = sim_matrix[np.triu_indices(top_n, k=1)]\n",
    "    diversity = 1 - np.mean(upper_triangle)\n",
    "    diversities.append(diversity)\n",
    "\n",
    "    # Serendipity\n",
    "    liked_items = np.where(ratings_filled[user_idx] >= 4)[0]\n",
    "    liked_vecs = item_factors[liked_items]\n",
    "\n",
    "    ser_scores = []\n",
    "    for i in top_items:\n",
    "        rec_vec = item_factors[i].reshape(1, -1)\n",
    "        if liked_vecs.shape[0] > 0:\n",
    "            sim = cosine_similarity(rec_vec, liked_vecs)\n",
    "            ser_scores.append(1 - np.mean(sim))\n",
    "    serendipity = np.mean(ser_scores) if ser_scores else 0\n",
    "    serendipities.append(serendipity)\n",
    "\n",
    "# Final average metrics\n",
    "avg_novelty = np.mean(novelties)\n",
    "avg_diversity = np.mean(diversities)\n",
    "avg_serendipity = np.mean(serendipities)\n",
    "\n",
    "print(f\"\\nAverage over all users:\")\n",
    "print(f\"Novelty: {avg_novelty:.4f}\")\n",
    "print(f\"Diversity: {avg_diversity:.4f}\")\n",
    "print(f\"Serendipity: {avg_serendipity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1292d3",
   "metadata": {},
   "source": [
    "For all users serendipity metric is zero (or very low), it means the recommended items are too similar to what the users already liked, so there's little surprise or novelty in the recommendations relative to the user's usual preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f6a3d",
   "metadata": {},
   "source": [
    "Increase serendipity\n",
    "Serendipity was increased in this version, serendipity is based on low similarity to liked items, which reflects the \"pleasant surprise\" quality. low_sim = sim[sim < 0.7] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7843360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|██████████| 500/500 [00:00<00:00, 5506.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Serendipity (updated): 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming user_item_matrix is your DataFrame with users as index and movies as columns\n",
    "\n",
    "# Fill NaNs with zeros for SVD input\n",
    "ratings_filled = user_item_matrix.fillna(0).to_numpy()\n",
    "user_ids = user_item_matrix.index\n",
    "movie_ids = user_item_matrix.columns.to_numpy()\n",
    "\n",
    "# Train SVD\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(ratings_filled)  # shape: (num_users, 20)\n",
    "item_factors = svd.components_.T  # shape: (num_movies, 20)\n",
    "\n",
    "top_n = 10\n",
    "serendipities = []\n",
    "\n",
    "for user_idx in tqdm(range(len(user_ids)), desc=\"Evaluating users\"):\n",
    "    user_vector = user_factors[user_idx]\n",
    "    scores = np.dot(item_factors, user_vector)\n",
    "\n",
    "    liked_items = np.where(ratings_filled[user_idx] >= 4)[0]\n",
    "    all_items = np.arange(len(movie_ids))\n",
    "    candidate_items = np.setdiff1d(all_items, liked_items)\n",
    "\n",
    "    candidate_scores = scores[candidate_items]\n",
    "    top_candidate_idx = np.argsort(-candidate_scores)[:top_n]\n",
    "    top_items = candidate_items[top_candidate_idx]\n",
    "\n",
    "    liked_vecs = item_factors[liked_items]\n",
    "\n",
    "    ser_scores = []\n",
    "    for i in top_items:\n",
    "        rec_vec = item_factors[i].reshape(1, -1)\n",
    "        if liked_vecs.shape[0] > 0:\n",
    "            sim = cosine_similarity(rec_vec, liked_vecs).flatten()\n",
    "            # Consider only low similarity to liked items (less than 0.7)\n",
    "            low_sim = sim[sim < 0.7]\n",
    "            if len(low_sim) > 0:\n",
    "                ser_scores.append(1 - np.mean(low_sim))\n",
    "            else:\n",
    "                ser_scores.append(0)\n",
    "        else:\n",
    "            ser_scores.append(0.5)  # no liked items, neutral score\n",
    "\n",
    "    serendipity = np.mean(ser_scores) if ser_scores else 0\n",
    "    serendipities.append(serendipity)\n",
    "\n",
    "avg_serendipity = np.mean(serendipities)\n",
    "print(f\"Avg Serendipity (updated): {avg_serendipity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4598f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serendipity for User 48 (top 10 recommendations): 0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assume user_item_matrix is your DataFrame: users as index, movies as columns with ratings\n",
    "\n",
    "# Prepare filled ratings matrix for SVD\n",
    "ratings_filled = user_item_matrix.fillna(0).to_numpy()\n",
    "user_ids = user_item_matrix.index.to_list()\n",
    "movie_ids = user_item_matrix.columns.to_numpy()\n",
    "\n",
    "# Train SVD model\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "user_factors = svd.fit_transform(ratings_filled)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "# Get index of user 48 in the matrix\n",
    "try:\n",
    "    user_idx = user_ids.index(48)\n",
    "except ValueError:\n",
    "    raise ValueError(\"User 48 not found in user_item_matrix index\")\n",
    "\n",
    "# Compute scores for all items for user 48\n",
    "user_vector = user_factors[user_idx]\n",
    "scores = np.dot(item_factors, user_vector)\n",
    "\n",
    "# Identify items user 48 has liked (rating >= 4)\n",
    "liked_items = np.where(ratings_filled[user_idx] >= 4)[0]\n",
    "\n",
    "# Candidate items: items user 48 hasn't rated or rated less than 4\n",
    "all_items = np.arange(len(movie_ids))\n",
    "candidate_items = np.setdiff1d(all_items, liked_items)\n",
    "\n",
    "# Get scores for candidate items\n",
    "candidate_scores = scores[candidate_items]\n",
    "\n",
    "top_n = 10\n",
    "top_candidate_idx = np.argsort(-candidate_scores)[:top_n]\n",
    "top_items = candidate_items[top_candidate_idx]\n",
    "\n",
    "# Calculate serendipity: how dissimilar recommended items are to liked items\n",
    "liked_vecs = item_factors[liked_items]\n",
    "\n",
    "ser_scores = []\n",
    "for i in top_items:\n",
    "    rec_vec = item_factors[i].reshape(1, -1)\n",
    "    if liked_vecs.shape[0] > 0:\n",
    "        sim = cosine_similarity(rec_vec, liked_vecs).flatten()\n",
    "        low_sim = sim[sim < 0.7]  # threshold for “surprise”\n",
    "        if len(low_sim) > 0:\n",
    "            ser_scores.append(1 - np.mean(low_sim))\n",
    "        else:\n",
    "            ser_scores.append(0)\n",
    "    else:\n",
    "        ser_scores.append(0.5)  # no liked items, assign neutral serendipity\n",
    "\n",
    "serendipity_user_48 = np.mean(ser_scores) if ser_scores else 0\n",
    "\n",
    "print(f\"Serendipity for User 48 (top {top_n} recommendations): {serendipity_user_48:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42296dce",
   "metadata": {},
   "source": [
    "NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd37bb",
   "metadata": {},
   "source": [
    "TDIF SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c2c82",
   "metadata": {},
   "source": [
    " TF-IDF similarity results for movies similar to Toy Story.\n",
    " TF-IDF (Term Frequency–Inverse Document Frequency) compareD the textual content (plot summaries, keywords, etc.) of movies Similar to Toy Story. Based on this:\n",
    "\n",
    "Small Soldiers is the top match at 39%. It features toys coming to life, much like Toy Story, but with a military/sci-fi twist.\n",
    "\n",
    "Toy Story 2 being similar makes perfect sense at 35%. Tt's a direct sequel with near-identical characters and themes.\n",
    "\n",
    "The Indian in the Cupboard at 31%, also shares the theme of toys/figures coming to life and interacting with a child.\n",
    "\n",
    "Toys and Babes in Toyland at 23% carries themes centered around toy worlds or imaginative play, aligning them conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fdd8df",
   "metadata": {},
   "source": [
    "COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162275e",
   "metadata": {},
   "source": [
    "CountVectorizer similarity results for movies similar to Toy Story.\n",
    "\n",
    "CountVectorizer converts text (e.g., movie plot summaries) into a matrix of token counts.\n",
    "\n",
    "It builds a bag-of-words representation, capturing how often each word appears, but ignores word order and semantic meaning.\n",
    "\n",
    "Similarity is then computed using cosine similarity between these count vectors.\n",
    "\n",
    "According to  CountVectorizer Recommendations the similarity of other movies to Toy Story is very low. The highest recommended movie is Toy story 2 that is 45% similar, Followed by Small soldiers at 38%. CountVectorizer returns a lower similarity at 28% compared to TD-IDF that recommended the same movie at 31%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597fcab",
   "metadata": {},
   "source": [
    "COMPARISON BETWEEN TD-IDF and CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b66472",
   "metadata": {},
   "source": [
    "\n",
    "CountVectorizer treats plots as just word counts, ignoring word order or semantics.\n",
    "\n",
    "CountVectorizer scores don’t mean thematic or genre similarity,only textual overlap in description/metadata.\n",
    "\n",
    "According to CountVectorizer Toy Story is weakly similar to other 1995 films based.\n",
    "\n",
    "TF-IDF gives more refined similarity (as you saw earlier), while CountVectorizer tends to inflate common word matches.\n",
    "\n",
    "Using TF-IDF + cosine offers deeper semantic similarity.\n",
    "\n",
    "Both have 3 movies in common as highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a8116",
   "metadata": {},
   "source": [
    "COMPARISON BETWEEN User based CF recommendations and Item-Based CF Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d8958",
   "metadata": {},
   "source": [
    "The User based CF recommendation score is the predicted rating for an unseen movie, based on users similar to User 48 ratings, adjusted to the target user’s scale. Weighted average of normalized ratings from similar users was re-centered to user. UBCF recommended movies with Higher scores that User 48 is more strongly predicted to like. How i learned to stop to worry was most highly rated at 3.8 score.\n",
    "\n",
    "The Top 50 Item-Based CF Recommendations for User 48 score above is the predicted rating for an unseen movie, based on how similar it is to the movies the user 48 has already rated. Weighted average of user’s ratings for similar items was used. Compared to UBCF, Item based recommended movies have Lower scores Midnight Cowboy has a predicated rating of 3.4 meaning User 48 is predicted to like Item based recommendations less.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82709dea",
   "metadata": {},
   "source": [
    "COMPARISON BETWEEN User based CF RMSE and Item-Based CF RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96053d",
   "metadata": {},
   "source": [
    "The UBCF RMSE 0.8589 is Very good, it indicates the predicted ratings are close to actual ratings. It is lower than IBCF RMSE 0.967\n",
    "Precision@10 at 0.8492\tis Excellent because 84% of the top-10 recommendations are relevant (liked by the user).\n",
    "Recall@10 of 0.2670\tis decent because 26% of all relevant items are retrieved in the top-10. Low recall is common in sparse data.\n",
    "NDCG@10\t0.8652 is very strong, the relevant items are ranked high in the list, indicating great ordering of recommendations.\n",
    "\n",
    "The IBCF RMSE 0.9676 is Very good, it indicates the predicted ratings are close to actual ratings (low error).\n",
    "Precision@10 at 0.7942\tis Excellent because 79% of the top-10 recommendations are relevant (liked by the user).\n",
    "Recall@10 of 0.2452\tis decent because 24% of all relevant items are retrieved in the top-10. Low recall is common in sparse data.\n",
    "NDCG@10\t0.8013 is very strong, the relevant items are ranked high in the list, indicating great ordering of recommendations.\n",
    "\n",
    "\n",
    "COMPARISON BETWEEN UBCF AND IBCF METRICS\n",
    "User-Based CF outperforms Item-Based CF across all metrics in this case.\n",
    "UBCF has Lower RMSE hence more accurate predictions.\n",
    "\n",
    "Higher Precision and NDCG meaning more relevant and better-ordered top recommendations.\n",
    "\n",
    "Higher Recall that translates to more comprehensive retrieval of liked items.\n",
    "\n",
    "ALS has the lowest RMSE of 0.8363, meaning it's the most accurate model for predicting ratings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abed08a",
   "metadata": {},
   "source": [
    "DIVERSITY, NOVELTY, SERENDIPITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea9a3b",
   "metadata": {},
   "source": [
    "For user 50 Metrics\n",
    "Novelty\t0.477 is higher than CF models – ALS recommends less popular items\n",
    "Diversity of 0.6112 Indicates decent dissimilarity among top items\n",
    "Serendipity (increased) 0.50: Indicates Many recommended items are somewhat different from what the user already liked. \n",
    "\n",
    "\n",
    "Serendipity was increased because it was originally 0, serendipity is based on low similarity to liked items, which reflects the \"pleasant surprise\" quality. low_sim = sim[sim < 0.7]. \n",
    "\n",
    "A 0.5 Serendipity is a moderate serendipity score, indicating that:\n",
    "\n",
    "REcommendations are striking a balance between relevance and surprise.\n",
    "\n",
    "It’s higher than 0, which means ( adding a similarity threshold like < 0.7) helped introduce unexpected but still relevant items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb332562",
   "metadata": {},
   "source": [
    "In this first approach, serendipity was just computed by penalizing all high-similarity items,even if some surprise could’ve been good. All recommendations were similar to what the user already liked, I got 0 serendipity. np.mean(similarity_to_liked_items)\n",
    "\n",
    "Novelty of 0.3871 for the top-10 recommendations for each user include items that aren’t too popular.\n",
    "\n",
    "Since the score is close to 0.4, it means not just  blockbusters were recommended, but includes moderately obscure items.\n",
    "\n",
    "A perfect novelty (closer to 1.0) would mean mostly unknown or unrated items.\n",
    "\n",
    "A Diversity of 0.7006 is strong diversity, the recommended items for each user are not overly similar to each other.\n",
    "\n",
    "Diversity above 0.7 is generally considered good, especially for top-10 lists.\n",
    "\n",
    "It means users are seeing a range of genres, styles, or content types rather than a narrow cluster of similar items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
