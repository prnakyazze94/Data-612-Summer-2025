{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e131bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4a5c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0                         1                             2     3   \\\n",
      "0  movieId                     title                        genres  year   \n",
      "1        1          Toy Story (1995)   Animation|Children's|Comedy  1995   \n",
      "2        2            Jumanji (1995)  Adventure|Children's|Fantasy  1995   \n",
      "3        3   Grumpier Old Men (1995)                Comedy|Romance  1995   \n",
      "4        4  Waiting to Exhale (1995)                  Comedy|Drama  1995   \n",
      "\n",
      "                  4        5   \\\n",
      "0        clean_title  tmdb_id   \n",
      "1          Toy Story    862.0   \n",
      "2            Jumanji   8844.0   \n",
      "3   Grumpier Old Men  15602.0   \n",
      "4  Waiting to Exhale  31357.0   \n",
      "\n",
      "                                                  6   \\\n",
      "0                                           overview   \n",
      "1  Led by Woody, Andy's toys live happily in his ...   \n",
      "2  When siblings Judy and Peter discover an encha...   \n",
      "3  A family wedding reignites the ancient feud be...   \n",
      "4  Cheated on, mistreated and stepped on, the wom...   \n",
      "\n",
      "                                                  7   \\\n",
      "0                                        poster_path   \n",
      "1  https://image.tmdb.org/t/p/w500/uXDfjJbdP4ijW5...   \n",
      "2  https://image.tmdb.org/t/p/w500/p67m5dzwyxWd46...   \n",
      "3  https://image.tmdb.org/t/p/w500/1FSXpj5e8l4KH6...   \n",
      "4  https://image.tmdb.org/t/p/w500/qJU6rfil5xLVb5...   \n",
      "\n",
      "                                                  8             9   \\\n",
      "0                                      backdrop_path  vote_average   \n",
      "1  https://image.tmdb.org/t/p/w500/3Rfvhy1Nl6sSGJ...           8.0   \n",
      "2  https://image.tmdb.org/t/p/w500/pYw10zrqfkdm3y...         7.238   \n",
      "3  https://image.tmdb.org/t/p/w500/1o4vuCHpmd4DXo...         6.461   \n",
      "4  https://image.tmdb.org/t/p/w500/jZjoEKXMTDoZAG...           6.3   \n",
      "\n",
      "           10                                    11  \\\n",
      "0  vote_count                           tmdb_genres   \n",
      "1     18939.0  Animation, Adventure, Family, Comedy   \n",
      "2     10806.0            Adventure, Fantasy, Family   \n",
      "3       399.0                       Romance, Comedy   \n",
      "4       173.0                Comedy, Drama, Romance   \n",
      "\n",
      "                                                12               13  \\\n",
      "0                                       top_3_cast        directors   \n",
      "1                Tom Hanks, Tim Allen, Don Rickles    John Lasseter   \n",
      "2    Robin Williams, Kirsten Dunst, Bradley Pierce     Joe Johnston   \n",
      "3         Walter Matthau, Jack Lemmon, Ann-Margret    Howard Deutch   \n",
      "4  Whitney Houston, Angela Bassett, Loretta Devine  Forest Whitaker   \n",
      "\n",
      "                                                  14  \\\n",
      "0                                           keywords   \n",
      "1  rescue, friendship, mission, jealousy, villain...   \n",
      "2  giant insect, board game, disappearance, jungl...   \n",
      "3  fishing, sequel, old man, best friend, wedding...   \n",
      "4  based on novel or book, single mother, divorce...   \n",
      "\n",
      "                                            15  \n",
      "0                                 trailer_link  \n",
      "1  https://www.youtube.com/watch?v=CxwTLktovTU  \n",
      "2  https://www.youtube.com/watch?v=veszTagaXik  \n",
      "3  https://www.youtube.com/watch?v=rEnOoWs3FuA  \n",
      "4  https://www.youtube.com/watch?v=j9xml1CxgXI  \n",
      "Ratings:\n",
      "   userId  movieId  rating  timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "\n",
      "Users:\n",
      "   userId gender  age  occupation    zip\n",
      "0       1      F    1          10  48067\n",
      "1       2      M   56          16  70072\n",
      "2       3      M   25          15  55117\n",
      "3       4      M   45           7  02460\n",
      "4       5      M   25          20  55455\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\pricc\\Downloads\\movies_enriched_full.csv\"\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "movie_df = (\"movies_enriched_full.csv\")\n",
    "\n",
    "# File paths\n",
    "ratings_path = r\"C:\\Users\\pricc\\Downloads\\ratings.dat\"\n",
    "users_path = r\"C:\\Users\\pricc\\Downloads\\users.dat\"\n",
    "\n",
    "# Load ratings.dat\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Load users.dat\n",
    "users = pd.read_csv(\n",
    "    users_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    ")\n",
    "\n",
    "# Preview the data\n",
    "print(\"Ratings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nUsers:\")\n",
    "print(users.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37b0ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models built and similarity matrices saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load Enriched Movie Data\n",
    "# ----------------------------------------\n",
    "df = pd.read_csv(\"movies_enriched_full.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# SCombine Metadata Fields\n",
    "# ----------------------------------------\n",
    "\n",
    "def combine_metadata(row):\n",
    "    return \" \".join([\n",
    "        str(row[\"tmdb_genres\"]) if pd.notnull(row[\"tmdb_genres\"]) else \"\",\n",
    "        str(row[\"keywords\"]) if pd.notnull(row[\"keywords\"]) else \"\",\n",
    "        str(row[\"top_3_cast\"]) if pd.notnull(row[\"top_3_cast\"]) else \"\",\n",
    "        str(row[\"directors\"]) if pd.notnull(row[\"directors\"]) else \"\"\n",
    "    ]).lower().replace(\",\", \" \").replace(\":\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "df[\"metadata\"] = df.apply(combine_metadata, axis=1)\n",
    "\n",
    "# ----------------------------------------\n",
    "#  Build Vectorizers\n",
    "# ----------------------------------------\n",
    "\n",
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(df[\"metadata\"])\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"metadata\"])\n",
    "\n",
    "# ----------------------------------------\n",
    "#  Compute Cosine Similarity\n",
    "# ----------------------------------------\n",
    "\n",
    "cosine_sim_count = cosine_similarity(count_matrix)\n",
    "cosine_sim_tfidf = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# ----------------------------------------\n",
    "#  Save Results\n",
    "# ----------------------------------------\n",
    "\n",
    "# Save similarity matrices as NumPy arrays\n",
    "np.save(\"cosine_sim_count.npy\", cosine_sim_count)\n",
    "np.save(\"cosine_sim_tfidf.npy\", cosine_sim_tfidf)\n",
    "\n",
    "# Optional: Save similarity matrices as CSVs\n",
    "pd.DataFrame(cosine_sim_count, index=df[\"title\"], columns=df[\"title\"]).to_csv(\"cosine_sim_count.csv\")\n",
    "pd.DataFrame(cosine_sim_tfidf, index=df[\"title\"], columns=df[\"title\"]).to_csv(\"cosine_sim_tfidf.csv\")\n",
    "\n",
    "print(\" Models built and similarity matrices saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f81fd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39a43e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 similar movies using TF-IDF:\n",
      "Small Soldiers (1998) (Score: 0.3903)\n",
      "Toy Story 2 (1999) (Score: 0.3539)\n",
      "Indian in the Cupboard, The (1995) (Score: 0.3101)\n",
      "Toys (1992) (Score: 0.2656)\n",
      "Babes in Toyland (1961) (Score: 0.2394)\n"
     ]
    }
   ],
   "source": [
    "def recommend_movies(title, similarity_matrix, df, top_n=10):\n",
    "    idx = df[df[\"title\"] == title].index[0]\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    similar_movies = [(df.iloc[i][\"title\"], score) for i, score in sim_scores]\n",
    "    return similar_movies\n",
    "\n",
    "# Example: Recommend movies similar to \"Toy Story (1995)\"\n",
    "print(\"\\nTop 5 similar movies using TF-IDF:\")\n",
    "for movie, score in recommend_movies(\"Toy Story (1995)\", cosine_sim_tfidf, df, top_n=5):\n",
    "    print(f\"{movie} (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d2e73",
   "metadata": {},
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b5ea6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine Similarity Matrix (CountVectorizer):\n",
      "title                               Toy Story (1995)  Jumanji (1995)  \\\n",
      "title                                                                  \n",
      "Toy Story (1995)                            1.000000        0.054433   \n",
      "Jumanji (1995)                              0.054433        1.000000   \n",
      "Grumpier Old Men (1995)                     0.054433        0.000000   \n",
      "Waiting to Exhale (1995)                    0.066227        0.064889   \n",
      "Father of the Bride Part II (1995)          0.046676        0.034300   \n",
      "\n",
      "title                               Grumpier Old Men (1995)  \\\n",
      "title                                                         \n",
      "Toy Story (1995)                                   0.054433   \n",
      "Jumanji (1995)                                     0.000000   \n",
      "Grumpier Old Men (1995)                            1.000000   \n",
      "Waiting to Exhale (1995)                           0.097333   \n",
      "Father of the Bride Part II (1995)                 0.068599   \n",
      "\n",
      "title                               Waiting to Exhale (1995)  \\\n",
      "title                                                          \n",
      "Toy Story (1995)                                    0.066227   \n",
      "Jumanji (1995)                                      0.064889   \n",
      "Grumpier Old Men (1995)                             0.097333   \n",
      "Waiting to Exhale (1995)                            1.000000   \n",
      "Father of the Bride Part II (1995)                  0.027821   \n",
      "\n",
      "title                               Father of the Bride Part II (1995)  \n",
      "title                                                                   \n",
      "Toy Story (1995)                                              0.046676  \n",
      "Jumanji (1995)                                                0.034300  \n",
      "Grumpier Old Men (1995)                                       0.068599  \n",
      "Waiting to Exhale (1995)                                      0.027821  \n",
      "Father of the Bride Part II (1995)                            1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load movie data and similarity matrices\n",
    "df = pd.read_csv(\"movies_enriched_full.csv\")\n",
    "cosine_sim_count = np.load(\"cosine_sim_count.npy\")\n",
    "cosine_sim_tfidf = np.load(\"cosine_sim_tfidf.npy\")\n",
    "print(\" Cosine Similarity Matrix (CountVectorizer):\")\n",
    "print(pd.DataFrame(cosine_sim_count, index=df[\"title\"], columns=df[\"title\"]).iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238fa74",
   "metadata": {},
   "source": [
    "Toy story has 1.000000 self-similarity to Toy Story\n",
    "\n",
    "0.054433 is how similar Toy Story is to Jumanji\n",
    "\n",
    "0.000000 means no content overlap between the movies for example Grumpier Old Men and Jumanji have no overlapping keywords/cast/etc.\n",
    "\n",
    "Waiting to Exhale has 6.6% similarity to Toy Story under CountVectoriz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972dcb1",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c5a30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cosine Similarity Matrix (TF-IDF):\n",
      "title                               Toy Story (1995)  Jumanji (1995)  \\\n",
      "title                                                                  \n",
      "Toy Story (1995)                            1.000000        0.014150   \n",
      "Jumanji (1995)                              0.014150        1.000000   \n",
      "Grumpier Old Men (1995)                     0.022040        0.000000   \n",
      "Waiting to Exhale (1995)                    0.028133        0.017912   \n",
      "Father of the Bride Part II (1995)          0.009242        0.009244   \n",
      "\n",
      "title                               Grumpier Old Men (1995)  \\\n",
      "title                                                         \n",
      "Toy Story (1995)                                   0.022040   \n",
      "Jumanji (1995)                                     0.000000   \n",
      "Grumpier Old Men (1995)                            1.000000   \n",
      "Waiting to Exhale (1995)                           0.017067   \n",
      "Father of the Bride Part II (1995)                 0.024845   \n",
      "\n",
      "title                               Waiting to Exhale (1995)  \\\n",
      "title                                                          \n",
      "Toy Story (1995)                                    0.028133   \n",
      "Jumanji (1995)                                      0.017912   \n",
      "Grumpier Old Men (1995)                             0.017067   \n",
      "Waiting to Exhale (1995)                            1.000000   \n",
      "Father of the Bride Part II (1995)                  0.003701   \n",
      "\n",
      "title                               Father of the Bride Part II (1995)  \n",
      "title                                                                   \n",
      "Toy Story (1995)                                              0.009242  \n",
      "Jumanji (1995)                                                0.009244  \n",
      "Grumpier Old Men (1995)                                       0.024845  \n",
      "Waiting to Exhale (1995)                                      0.003701  \n",
      "Father of the Bride Part II (1995)                            1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Cosine Similarity Matrix (TF-IDF):\")\n",
    "print(pd.DataFrame(cosine_sim_tfidf, index=df[\"title\"], columns=df[\"title\"]).iloc[:5, :5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f4908",
   "metadata": {},
   "source": [
    "Print Top 5 Similar Movies for a Given Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb46e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations(title, similarity_matrix, df, top_n=5):\n",
    "    idx = df[df[\"title\"] == title].index[0]\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    print(f\"\\n Top {top_n} similar movies to '{title}':\")\n",
    "    for i, (movie_idx, score) in enumerate(sim_scores, 1):\n",
    "        print(f\"{i}. {df.iloc[movie_idx]['title']} (Similarity: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "241591e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Recommendations\n",
      "\n",
      " Top 5 similar movies to 'Toy Story (1995)':\n",
      "1. Small Soldiers (1998) (Similarity: 0.3903)\n",
      "2. Toy Story 2 (1999) (Similarity: 0.3539)\n",
      "3. Indian in the Cupboard, The (1995) (Similarity: 0.3101)\n",
      "4. Toys (1992) (Similarity: 0.2656)\n",
      "5. Babes in Toyland (1961) (Similarity: 0.2394)\n",
      " CountVectorizer Recommendations\n",
      "\n",
      " Top 5 similar movies to 'Toy Story (1995)':\n",
      "1. Toy Story 2 (1999) (Similarity: 0.4518)\n",
      "2. Small Soldiers (1998) (Similarity: 0.3790)\n",
      "3. Indian in the Cupboard, The (1995) (Similarity: 0.2887)\n",
      "4. Big (1988) (Similarity: 0.2502)\n",
      "5. Babes in Toyland (1961) (Similarity: 0.2485)\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF Recommendations')\n",
    "print_recommendations(\"Toy Story (1995)\", cosine_sim_tfidf, df, top_n=5)\n",
    "\n",
    "print(' CountVectorizer Recommendations')\n",
    "print_recommendations(\"Toy Story (1995)\", cosine_sim_count, df, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dff41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d271c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based CF Evaluation:\n",
      "  RMSE: nan\n",
      "  Precision@10: 0.0000\n",
      "  Recall@10: 0.0000\n",
      "  NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Memory-Based Collaborative Filtering (Bias-Normalized)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# --- Step 1: Create Mean-Centered User-Item Matrix ---\n",
    "def create_normalized_user_item_matrix(ratings):\n",
    "    matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_means = matrix.mean(axis=1)\n",
    "    return matrix.sub(user_means, axis=0).fillna(0), user_means\n",
    "\n",
    "# --- Step 2: Compute Cosine Similarity ---\n",
    "def compute_similarity(matrix, kind='user'):\n",
    "    if kind == 'user':\n",
    "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
    "    elif kind == 'item':\n",
    "        sim = 1 - pairwise_distances(matrix.T, metric='cosine')\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
    "    return sim\n",
    "\n",
    "# --- Step 3: Recommend Items ---\n",
    "def recommend_memory_based(user_id, user_item_matrix, user_means, similarity_matrix, kind='user', top_n=10):\n",
    "    model_label = f\"{kind.title()}-Based CF\"\n",
    "\n",
    "    if kind == 'user':\n",
    "        user_sim_scores = similarity_matrix[user_id - 1]\n",
    "        normalized_ratings = user_item_matrix.values\n",
    "        weighted_scores = user_sim_scores @ normalized_ratings\n",
    "        sum_weights = np.abs(user_sim_scores).sum()\n",
    "        if sum_weights == 0:\n",
    "            return pd.DataFrame(columns=['userId', 'movieId', 'score', 'model'])\n",
    "        predicted_ratings = weighted_scores / sum_weights\n",
    "        user_seen = user_item_matrix.loc[user_id]\n",
    "        unseen_mask = user_seen == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\n",
    "        recs += user_means.loc[user_id]\n",
    "\n",
    "    elif kind == 'item':\n",
    "        user_ratings = user_item_matrix.loc[user_id]\n",
    "        scores = user_ratings @ similarity_matrix\n",
    "        sum_weights = (user_ratings != 0) @ np.abs(similarity_matrix)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            predicted_ratings = np.true_divide(scores, sum_weights)\n",
    "            predicted_ratings[sum_weights == 0] = 0\n",
    "        unseen_mask = user_ratings == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
    "\n",
    "    recs = recs.sort_values(ascending=False).head(top_n)\n",
    "    return pd.DataFrame({\n",
    "        'userId': [user_id] * len(recs),\n",
    "        'movieId': recs.index,\n",
    "        'score': recs.values,\n",
    "        'model': model_label\n",
    "    })\n",
    "\n",
    "# --- Step 4: Evaluation Metrics ---\n",
    "def compute_rmse(preds, truth):\n",
    "    mask = ~truth.isna()\n",
    "    mse = ((truth[mask] - preds[mask]) ** 2).mean().mean()\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "def precision_recall_ndcg_at_k(preds, truth, k=10, threshold=4.0):\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for uid in preds.index:\n",
    "        if uid not in truth.index:\n",
    "            continue\n",
    "        pred_scores = preds.loc[uid].dropna().sort_values(ascending=False).head(k)\n",
    "        actual_ratings = truth.loc[uid]\n",
    "\n",
    "        relevant = actual_ratings[actual_ratings >= threshold].index\n",
    "        recommended = pred_scores.index\n",
    "\n",
    "        true_positives = len(set(recommended) & set(relevant))\n",
    "        precisions.append(true_positives / k)\n",
    "        recalls.append(true_positives / len(relevant) if len(relevant) else 0)\n",
    "\n",
    "        dcg = sum([\n",
    "            1 / np.log2(i + 2) if rec in relevant else 0\n",
    "            for i, rec in enumerate(recommended)\n",
    "        ])\n",
    "        idcg = sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)\n",
    "\n",
    "# --- Step 5: Helper to Convert to Prediction Matrix ---\n",
    "def to_pred_matrix(recs_df, users, items):\n",
    "    matrix = pd.DataFrame(index=users, columns=items)\n",
    "    for _, row in recs_df.iterrows():\n",
    "        matrix.at[row['userId'], row['movieId']] = row['score']\n",
    "    return matrix.astype(float)\n",
    "\n",
    "# === Load Ratings ===\n",
    "ratings = pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# === Build Matrix & Similarities ===\n",
    "user_item_matrix, user_means = create_normalized_user_item_matrix(ratings)\n",
    "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
    "item_sim_matrix = compute_similarity(user_item_matrix, kind='item')\n",
    "\n",
    "# === Generate Recommendations for All Users ===\n",
    "user_recs_all = []\n",
    "for uid in ratings['userId'].unique()[:100]:\n",
    "    recs = recommend_memory_based(uid, user_item_matrix, user_means, user_sim_matrix, kind='user', top_n=10)\n",
    "    user_recs_all.append(recs)\n",
    "user_recs_df = pd.concat(user_recs_all, ignore_index=True)\n",
    "\n",
    "# === Convert to Prediction Matrix ===\n",
    "all_users = ratings['userId'].unique()\n",
    "all_movies = ratings['movieId'].unique()\n",
    "pred_user_cf = to_pred_matrix(user_recs_df, all_users, all_movies)\n",
    "\n",
    "# === Build Truth Matrix ===\n",
    "truth = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# === Evaluate ===\n",
    "rmse_user = compute_rmse(pred_user_cf, truth)\n",
    "prec_user, rec_user, ndcg_user = precision_recall_ndcg_at_k(pred_user_cf, truth, k=10)\n",
    "\n",
    "print(f\"User-Based CF Evaluation:\")\n",
    "print(f\"  RMSE: {rmse_user:.4f}\")\n",
    "print(f\"  Precision@10: {prec_user:.4f}\")\n",
    "print(f\"  Recall@10: {rec_user:.4f}\")\n",
    "print(f\"  NDCG@10: {ndcg_user:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf09ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based similarity computed. Shape: (500, 500)\n",
      "User similarity computed in 0.03 sec\n",
      "Item-based similarity computed. Shape: (500, 500)\n",
      "Item similarity computed in 0.02 sec\n",
      "\n",
      "Top 50 User-Based CF Recommendations for User 48:\n",
      "   movieId                                              title     score\n",
      "0      750  Dr. Strangelove or: How I Learned to Stop Worr...  3.818245\n",
      "1      904                                 Rear Window (1954)  3.756734\n",
      "2      908                          North by Northwest (1959)  3.723104\n",
      "3     1089                              Reservoir Dogs (1992)  3.708471\n",
      "4     1234                                  Sting, The (1973)  3.702266\n",
      "\n",
      "Top 50 Item-Based CF Recommendations for User 48:\n",
      "   movieId                         title     score\n",
      "0      953  It's a Wonderful Life (1946)  0.433139\n",
      "1     1272                 Patton (1970)  0.418386\n",
      "2     1961               Rain Man (1988)  0.416326\n",
      "3     2329     American History X (1998)  0.413722\n",
      "4     1234             Sting, The (1973)  0.413218\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "\n",
    "# ==============================\n",
    "# Load and Subset Data\n",
    "# ==============================\n",
    "\n",
    "ratings =  pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(file_path)[['movieId', 'title']]\n",
    "\n",
    "# TEMP: Use a smaller subset for faster testing\n",
    "subset_users = ratings['userId'].value_counts().head(500).index\n",
    "subset_movies = ratings['movieId'].value_counts().head(500).index\n",
    "ratings_small = ratings[ratings['userId'].isin(subset_users) & ratings['movieId'].isin(subset_movies)]\n",
    "\n",
    "# ==============================\n",
    "# Step 1: Create Bias-Normalized Matrix\n",
    "# ==============================\n",
    "\n",
    "def create_normalized_user_item_matrix(ratings):\n",
    "    matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_means = matrix.mean(axis=1)\n",
    "    return matrix.sub(user_means, axis=0).fillna(0), user_means\n",
    "\n",
    "user_item_matrix, user_means = create_normalized_user_item_matrix(ratings_small)\n",
    "\n",
    "# ==============================\n",
    "# Step 2: Compute Similarity Matrices\n",
    "# ==============================\n",
    "\n",
    "def compute_similarity(matrix, kind='user'):\n",
    "    if kind == 'user':\n",
    "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
    "    elif kind == 'item':\n",
    "        sim = 1 - pairwise_distances(matrix.T, metric='cosine')\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
    "    print(f\"{kind.title()}-based similarity computed. Shape: {sim.shape}\")\n",
    "    return sim\n",
    "\n",
    "start = time.time()\n",
    "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
    "print(f\"User similarity computed in {time.time() - start:.2f} sec\")\n",
    "\n",
    "start = time.time()\n",
    "item_sim_matrix = compute_similarity(user_item_matrix, kind='item')\n",
    "print(f\"Item similarity computed in {time.time() - start:.2f} sec\")\n",
    "\n",
    "# ==============================\n",
    "# Step 3: Recommendation Function\n",
    "# ==============================\n",
    "\n",
    "def recommend_memory_based(user_id, user_item_matrix, user_means, similarity_matrix, kind='user', top_n=50):\n",
    "    model_label = f\"{kind.title()}-Based CF\"\n",
    "\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        print(f\"User {user_id} not in matrix.\")\n",
    "        return pd.DataFrame(columns=['movieId', 'score', 'model'])\n",
    "\n",
    "    if kind == 'user':\n",
    "        user_sim_scores = similarity_matrix[user_item_matrix.index.get_loc(user_id)]\n",
    "        normalized_ratings = user_item_matrix.values\n",
    "\n",
    "        weighted_scores = user_sim_scores @ normalized_ratings\n",
    "        sum_weights = np.abs(user_sim_scores).sum()\n",
    "\n",
    "        if sum_weights == 0:\n",
    "            print(\"No similar users found.\")\n",
    "            return pd.DataFrame(columns=['movieId', 'score', 'model'])\n",
    "\n",
    "        predicted_ratings = weighted_scores / sum_weights\n",
    "        user_seen = user_item_matrix.loc[user_id]\n",
    "        unseen_mask = user_seen == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\\\n",
    "            .sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        recs += user_means.loc[user_id]\n",
    "\n",
    "    elif kind == 'item':\n",
    "        user_ratings = user_item_matrix.loc[user_id]\n",
    "        scores = user_ratings @ similarity_matrix\n",
    "        sum_weights = (user_ratings != 0) @ np.abs(similarity_matrix)\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            predicted_ratings = np.true_divide(scores, sum_weights)\n",
    "            predicted_ratings[sum_weights == 0] = 0\n",
    "\n",
    "        unseen_mask = user_ratings == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask]\\\n",
    "            .sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'\") \n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'movieId': recs.index,\n",
    "        'score': recs.values,\n",
    "        'model': model_label\n",
    "    })\n",
    "\n",
    "# ==============================\n",
    "# Step 4: Generate and Merge Recommendations\n",
    "# ==============================\n",
    "\n",
    "test_user_id = user_item_matrix.index[0]  # Just pick the first user in the small set\n",
    "\n",
    "user_cf_recs = recommend_memory_based(\n",
    "    test_user_id, user_item_matrix, user_means, user_sim_matrix, kind='user', top_n=50\n",
    ")\n",
    "\n",
    "item_cf_recs = recommend_memory_based(\n",
    "    test_user_id, user_item_matrix, user_means, item_sim_matrix, kind='item', top_n=50\n",
    ")\n",
    "\n",
    "user_cf_recs = user_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
    "item_cf_recs = item_cf_recs.merge(movies, on=\"movieId\", how=\"left\")\n",
    "\n",
    "# ==============================\n",
    "# Step 5: Display\n",
    "# ==============================\n",
    "\n",
    "print(f\"\\nTop 50 User-Based CF Recommendations for User {test_user_id}:\")\n",
    "print(user_cf_recs[['movieId', 'title', 'score']].head())\n",
    "\n",
    "print(f\"\\nTop 50 Item-Based CF Recommendations for User {test_user_id}:\")\n",
    "print(item_cf_recs[['movieId', 'title', 'score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e5c867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based CF Recommendations:\n",
      "   movieId     score          model\n",
      "0      750  3.818245  User-Based CF\n",
      "1      904  3.756734  User-Based CF\n",
      "2      908  3.723104  User-Based CF\n",
      "3     1089  3.708471  User-Based CF\n",
      "4     1234  3.702266  User-Based CF\n",
      "\n",
      "Item-Based CF Recommendations:\n",
      "   movieId     score          model\n",
      "0      953  0.433139  Item-Based CF\n",
      "1     1272  0.418386  Item-Based CF\n",
      "2     1961  0.416326  Item-Based CF\n",
      "3     2329  0.413722  Item-Based CF\n",
      "4     1234  0.413218  Item-Based CF\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Load ratings\n",
    "ratings =  pd.read_csv(\n",
    "    ratings_path,\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Subset: Top 500 Users & Movies\n",
    "# ==============================\n",
    "subset_users = ratings['userId'].value_counts().head(500).index\n",
    "subset_movies = ratings['movieId'].value_counts().head(500).index\n",
    "ratings_small = ratings[ratings['userId'].isin(subset_users) & ratings['movieId'].isin(subset_movies)]\n",
    "\n",
    "# ==============================\n",
    "# Create Bias-Normalized Matrix\n",
    "# ==============================\n",
    "def create_normalized_user_item_matrix(ratings):\n",
    "    matrix = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_means = matrix.mean(axis=1)\n",
    "    return matrix.sub(user_means, axis=0).fillna(0), user_means\n",
    "\n",
    "user_item_matrix, user_means = create_normalized_user_item_matrix(ratings_small)\n",
    "\n",
    "# ==============================\n",
    "# Similarity Calculation\n",
    "# ==============================\n",
    "def compute_similarity(matrix, kind='user'):\n",
    "    if kind == 'user':\n",
    "        sim = 1 - pairwise_distances(matrix, metric='cosine')\n",
    "    elif kind == 'item':\n",
    "        sim = 1 - pairwise_distances(matrix.T, metric='cosine')\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'user' or 'item'\")\n",
    "    return sim\n",
    "\n",
    "user_sim_matrix = compute_similarity(user_item_matrix, kind='user')\n",
    "item_sim_matrix = compute_similarity(user_item_matrix, kind='item')\n",
    "\n",
    "# ==============================\n",
    "# Recommend Function\n",
    "# ==============================\n",
    "def recommend_memory_based(user_id, user_item_matrix, user_means, similarity_matrix, kind='user', top_n=10):\n",
    "    model_label = f\"{kind.title()}-Based CF\"\n",
    "\n",
    "    if kind == 'user':\n",
    "        user_sim_scores = similarity_matrix[user_item_matrix.index.get_loc(user_id)]\n",
    "        normalized_ratings = user_item_matrix.values\n",
    "\n",
    "        weighted_scores = user_sim_scores @ normalized_ratings\n",
    "        sum_weights = np.abs(user_sim_scores).sum()\n",
    "        if sum_weights == 0:\n",
    "            return pd.DataFrame(columns=['movieId', 'score', 'model'])\n",
    "\n",
    "        predicted_ratings = weighted_scores / sum_weights\n",
    "        user_seen = user_item_matrix.loc[user_id]\n",
    "        unseen_mask = user_seen == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask] \\\n",
    "            .sort_values(ascending=False).head(top_n)\n",
    "        recs += user_means.loc[user_id]\n",
    "\n",
    "    elif kind == 'item':\n",
    "        user_ratings = user_item_matrix.loc[user_id]\n",
    "        scores = user_ratings @ similarity_matrix\n",
    "        sum_weights = (user_ratings != 0) @ np.abs(similarity_matrix)\n",
    "        predicted_ratings = np.divide(scores, sum_weights, out=np.zeros_like(scores), where=sum_weights != 0)\n",
    "        unseen_mask = user_ratings == 0\n",
    "        recs = pd.Series(predicted_ratings, index=user_item_matrix.columns)[unseen_mask] \\\n",
    "            .sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'movieId': recs.index,\n",
    "        'score': recs.values,\n",
    "        'model': model_label\n",
    "    })\n",
    "\n",
    "# ==============================\n",
    "# Run Example\n",
    "# ==============================\n",
    "example_user_id = user_item_matrix.index[0]  # pick any from subset\n",
    "user_cf_recs = recommend_memory_based(example_user_id, user_item_matrix, user_means, user_sim_matrix, kind='user')\n",
    "item_cf_recs = recommend_memory_based(example_user_id, user_item_matrix, user_means, item_sim_matrix, kind='item')\n",
    "\n",
    "print(\"User-Based CF Recommendations:\")\n",
    "print(user_cf_recs.head())\n",
    "\n",
    "print(\"\\nItem-Based CF Recommendations:\")\n",
    "print(item_cf_recs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af432740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def compute_rmse(preds_df, truth_df):\n",
    "    # Align both dataframes by index and columns\n",
    "    preds_aligned, truth_aligned = preds_df.align(truth_df, join='inner', axis=0)\n",
    "    preds_aligned, truth_aligned = preds_aligned.align(truth_df, join='inner', axis=1)\n",
    "\n",
    "    # Flatten and keep only pairs where both values are not NaN\n",
    "    mask = (~preds_aligned.isna()) & (~truth_aligned.isna())\n",
    "    y_true = truth_aligned[mask].values\n",
    "    y_pred = preds_aligned[mask].values\n",
    "\n",
    "    if y_true.size == 0:\n",
    "        print(\"No overlapping ratings to compare.\")\n",
    "        return np.nan\n",
    "\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def precision_recall_ndcg_at_k(preds, truth, k=10):\n",
    "    \"\"\"\n",
    "    Compute Precision@K, Recall@K, and NDCG@K.\n",
    "    \"\"\"\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "    for user in truth.index:\n",
    "        true_ratings = truth.loc[user].dropna()\n",
    "        if true_ratings.empty:\n",
    "            continue\n",
    "\n",
    "        true_top_k = set(true_ratings.sort_values(ascending=False).head(k).index)\n",
    "        pred_top_k = set(preds.loc[user].sort_values(ascending=False).head(k).index)\n",
    "\n",
    "        tp = len(true_top_k & pred_top_k)\n",
    "        prec = tp / k\n",
    "        rec = tp / len(true_top_k) if true_top_k else 0\n",
    "\n",
    "        dcg = sum([1 / np.log2(i+2) if m in true_top_k else 0 for i, m in enumerate(preds.loc[user].sort_values(ascending=False).head(k).index)])\n",
    "        idcg = sum([1 / np.log2(i+2) for i in range(min(k, len(true_top_k)))])\n",
    "        ndcg = dcg / idcg if idcg != 0 else 0\n",
    "\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(ndcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5883b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_userwise(df, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    For each user, split their ratings into train and test sets.\n",
    "    \"\"\"\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    for user_id, user_df in df.groupby('userId'):\n",
    "        user_df = user_df.sample(frac=1, random_state=42)\n",
    "        test_size = int(len(user_df) * test_ratio)\n",
    "        test_list.append(user_df.iloc[:test_size])\n",
    "        train_list.append(user_df.iloc[test_size:])\n",
    "\n",
    "    train_df = pd.concat(train_list)\n",
    "    test_df = pd.concat(test_list)\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c91c0371",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m truth_matrix \u001b[38;5;241m=\u001b[39m test_ratings\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieId\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m rmse_user \u001b[38;5;241m=\u001b[39m compute_rmse(pred_user_cf, truth_matrix)\n\u001b[0;32m     28\u001b[0m rmse_item \u001b[38;5;241m=\u001b[39m compute_rmse(pred_item_cf, truth_matrix)\n\u001b[0;32m     30\u001b[0m prec_user, rec_user, ndcg_user \u001b[38;5;241m=\u001b[39m precision_recall_ndcg_at_k(pred_user_cf, truth_matrix, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mcompute_rmse\u001b[1;34m(preds_df, truth_df)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo overlapping ratings to compare.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(mean_squared_error(y_true, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    507\u001b[0m     y_true, y_pred, multioutput\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:112\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m--> 112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pricc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_ratings, test_ratings = train_test_split_userwise(ratings_small, test_ratio=0.2)\n",
    "\n",
    "# Create train matrix\n",
    "user_item_train, user_means_train = create_normalized_user_item_matrix(train_ratings)\n",
    "user_sim = compute_similarity(user_item_train, kind='user')\n",
    "item_sim = compute_similarity(user_item_train, kind='item')\n",
    "\n",
    "# Predict ratings for test users\n",
    "test_users = test_ratings['userId'].unique()\n",
    "pred_user_cf = pd.DataFrame(index=user_item_train.index, columns=user_item_train.columns)\n",
    "pred_item_cf = pd.DataFrame(index=user_item_train.index, columns=user_item_train.columns)\n",
    "\n",
    "for user in test_users:\n",
    "    # User-based prediction\n",
    "    recs_user = recommend_memory_based(user, user_item_train, user_means_train, user_sim, kind='user', top_n=len(user_item_train.columns))\n",
    "    pred_user_cf.loc[user, recs_user['movieId']] = recs_user['score'].values\n",
    "\n",
    "    # Item-based prediction\n",
    "    recs_item = recommend_memory_based(user, user_item_train, user_means_train, item_sim, kind='item', top_n=len(user_item_train.columns))\n",
    "    pred_item_cf.loc[user, recs_item['movieId']] = recs_item['score'].values\n",
    "\n",
    "# Create ground truth matrix\n",
    "truth_matrix = test_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Evaluate\n",
    "rmse_user = compute_rmse(pred_user_cf, truth_matrix)\n",
    "rmse_item = compute_rmse(pred_item_cf, truth_matrix)\n",
    "\n",
    "prec_user, rec_user, ndcg_user = precision_recall_ndcg_at_k(pred_user_cf, truth_matrix, k=10)\n",
    "prec_item, rec_item, ndcg_item = precision_recall_ndcg_at_k(pred_item_cf, truth_matrix, k=10)\n",
    "\n",
    "print(f\"User-based CF:\\n RMSE={rmse_user:.4f}, Precision@10={prec_user:.4f}, Recall@10={rec_user:.4f}, NDCG@10={ndcg_user:.4f}\")\n",
    "print(f\"Item-based CF:\\n RMSE={rmse_item:.4f}, Precision@10={prec_item:.4f}, Recall@10={rec_item:.4f}, NDCG@10={ndcg_item:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
